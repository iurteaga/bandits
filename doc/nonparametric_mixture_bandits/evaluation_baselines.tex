% !TEX root = main.tex
\subsection{Thompson sampling-based state-of-the-art baselines}
\label{ssec:ts_baselines}

Thompson sampling provides a bandit framework that requires access to posterior samples of the reward model.
An approach to extend its applicability to complex domains is to leverage the advances in Bayesian neural networks and to merge them with approximate Bayesian inference methods. The algorithms listed below are state-of-the art techniques that provide different computations of (or approximations to) reward posteriors that can be combined with Thompson sampling to address contextual multi-armed bandits.

\begin{enumerate}
	\item \texttt{LinearGaussian TS}: A powerful (not neural network based) Thompson sampling baseline, which assumes a contextual linear Gaussian reward function,
	\begin{equation}
	Y_{t,a}\sim \N{Y|x_t^\top\theta_a, \sigma_a^2} \;. \nonumber
	\end{equation}
	In its simplest setting, when the reward variance is known, the resulting Thompson sampling implementation follows that by~\citet{ip-Agrawal2013a}. In our experiments, in a similar fashion as done by~\citet{ip-Riquelme2018}, we model the joint distribution of $\theta_a$ and $\sigma_a^2$, $\forall a \in \A$, which allows the method to adaptively adjust to the observed reward noise. We leverage the Normal-inverse Gamma conjugate prior, \ie
	\begin{equation}
	\left(\theta_a, \sigma_a^2\right) \sim \NIG{\theta_a, \sigma_a^2 |m_{a,0}, \Sigma_{a,0},\alpha_{a,0}, \beta_{a,0}} \; ,
	\end{equation}
	of the Gaussian contextual model to derive the exact Bayesian posterior. We use an uninformative prior for the variance ($\alpha_{a,0}=1, \beta_{a,0}=1, \forall a$) and a standard uncorrelated Gaussian for the mean prior ($m_{a,0}=0, \Sigma_{a,0}=I, \forall a$).

	\item \texttt{MultitaskGP}: This is an alternative and popular Bayesian nonparametric technique that models context to reward mappings via Gaussian processes~\citep{ip-Srinivas2010,ip-Gruenewaelder2010,ic-Krause2011}. This implementation regresses the expected reward of different context-actions pairs by fitting a multitask Gaussian process given observed bandit data.
	
	\item \texttt{NeuralLinear}: This algorithm, introduced by~\citet{ip-Riquelme2018}, operates by learning a neural network that maps contexts to rewards for each action, and simultaneously, updates a Bayesian linear regression in the network's last layer. The last layer maps a learned representation $z$ linearly to the rewards $y$. The corresponding Thompson sampling draws the learned linear regression parameters $\theta_a$ for each arm, but keeps the representation $z$ output by the learned network. In our experiments, the representation network and the Bayesian linear posterior are retrained and updated at every MAB interaction.

	\item \texttt{NeuralBootstrapped}: This algorithm is based on~\citep{ic-Osband2016}, which trains simultaneously (in parallel) $Q$ neural networks based on different bootstrapped bandit histories $\HH_{1:t}^{(1)},\cdots, \HH_{1:t}^{(Q)}$. These are generated by adding each newly observed evidence $(x_{t}, a_{t}, y_{t})$ to each history $\HH_{1:t-1}^{(q)}$, $q=\{1, \cdots,Q\}$, independently and with probability $p \in (0, 1]$. In order to choose an action for a given context, one of the $q$ networks is selected with uniform probability ($1/Q$), and the best action according to the selected network is played.
	
	\item \texttt{NeuralRMS}: This is a simple bandit benchmark that trains a neural network to map contexts to rewards. At each time $t$, it acts $\epsilon$-greedily according to the current model, which due to the Stochastic Gradient Descent (SGD) algorithm used for training (\ie the RMSProp optimizer in this implementation), captures randomness in its output.
	
	\item \texttt{NeuralDropoutRMS}: Dropout is a neural network training technique where the output of each neuron is independently zeroed out with a given probability in each forward pass. Once the network is trained, dropout can also be used to obtain a distribution of predictions for a specific input. By choosing the best action with respect to the random dropout prediction, an implicit form of Thompson sampling is implemented.
	
	\item \texttt{NeuralParamNoise}: An approach to approximate a distribution over neural networks consists in randomly perturbing the point estimates attained by SGD on the available data~\cite{ip-Plappert2018}. In this case, the model uses a heuristic to control the amount of \iid noise it adds to the parameters of the neural network, which is used for making a decision, but not for training steps: SGD re-starts from the last, noiseless parameter value.
	
	\item \texttt{BNNVariationalGaussian}: This algorithm is based on ideas presented in~\citep{ip-Blundell2015} that combine stochastic variational inference and Bayes by backpropagation. It implements a Bayesian neural network by modeling each individual weight posterior as a univariate Gaussian distribution. Thompson sampling then draws a network at each time step by sampling each weight independently. The variational approach consists in maximizing a proxy for the maximum likelihood estimates of the network weights given observed data, to fit the unknown parameters of the variational posterior.
	
	\item \texttt{BNNAlphaDiv}: This technique leverages expecta\-tion\-/pro\-pagation and Black\-/box alpha\-/divergence minimization as in~\cite{ip-Hernandez-Lobato2016} to approximate the unknown reward distribution. The algorithm iteratively approximates the posterior of interest by updating a single approximation factor at a time, which usually corresponds to the likelihood of one data point. The implementation adopted here optimizes the global objective directly via stochastic gradient descent.
	
	\item \texttt{Optimal}: When possible (\ie for simulated bandits for which there is ground truth), we implement an optimal multi-armed bandit policy that selects the best arm for the given context, based on the known true expected reward of each arm.
\end{enumerate}

As reported by~\citet{ip-Riquelme2018}, deep learning methods are very sensitive to the selection of a wide variety of hyperparameters, and these hyperparameter choices are known to be highly dataset dependent. Besides, in bandit scenarios, we do not have access to each problem to perform any a-priori tuning. Therefore, we resort to the default settings provided by the authors in \href{https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits}{their implementation}.

A full description of these algorithms and all implementation details can be found in the \href{https://sites.google.com/site/deepbayesianbandits/}{original deep bandit showdown work}.
For completeness, we present the set of hyperparameters used for our experiments in Section~\ref{asec:evaluation_hyperparameters} of the Appendix. In addition, and following the insights from~\cite{ip-Riquelme2018} that partially optimized uncertainty estimates can lead to catastrophic decisions with neural networks, we retrain the neural network models at every iteration of the multi-armed bandit ---note that since stochastic gradient descent is used for training, randomness is incorporated in all implementations.

