@InCollection{ic-Abbasi-Yadkori2011,
  Title                    = {{Improved Algorithms for Linear Stochastic Bandits}},
  Author                   = {Yasin Abbasi-Yadkori and P\'{a}l, D\'{a}vid and Csaba Szepesv\'{a}ri},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  Pages                    = {2312--2320},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {https://papers.nips.cc/paper/4417-improved-algorithms-for-linear-stochastic-bandits}
}

@Article{j-Arulampalam2002,
  Title                    = {{A tutorial on particle filters for online nonlinear/non-{G}aussian {B}ayesian tracking}},
  Author                   = {M. Sanjeev Arulampalam and Simon Maskell and Neil Gordon and Tim Clapp},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {2},
  Number                   = {2},
  Pages                    = {174-188},
  Volume                   = {50},

  Abstract                 = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or "particle") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example},
  ISSN                     = {1053-587X},
  Keywords                 = {Bayes methods;Kalman filters;Monte Carlo methods;filtering theory;importance sampling;state estimation;state-space methods;tracking filters;Kalman filtering;nonGaussian tracking problems;nonlinear tracking problems;optimal Bayesian algorithms;particle filters;point mass representations;probability densities;sequential Monte Carlo methods;sequential importance sampling;state-space model;suboptimal Bayesian algorithms;tutorial;Bayesian methods;Costs;Filtering;Kalman filters;Monte Carlo methods;Nonlinear dynamical systems;Particle filters;Particle tracking;Signal processing;Tutorial},
  Owner                    = {iurteaga},
  Timestamp                = {2015-11-03}
}

@Article{j-Auer2002,
  Title                    = {{Finite-time Analysis of the Multiarmed Bandit Problem}},
  Author                   = {Peter Auer and Nicol\`{o} Cesa-Bianchi and Paul Fischer},
  Journal                  = {Machine Learning},
  Year                     = {2002},

  Month                    = may,
  Number                   = {2-3},
  Pages                    = {235--256},
  Volume                   = {47},

  Acmid                    = {599677},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1023/A:1013689704352},
  ISSN                     = {0885-6125},
  Issue_date               = {May-June 2002},
  Keywords                 = {adaptive allocation rules, bandit problems, finite horizon regret},
  Numpages                 = {22},
  Owner                    = {iurteaga},
  Publisher                = {Kluwer Academic Publishers},
  Timestamp                = {2017.05.10}
}

@Article{j-Bellm1956,
  Title                    = {{A Problem in the Sequential Design of Experiments}},
  Author                   = {Richard Bellman},
  Journal                  = {Sankhya: The Indian Journal of Statistics (1933 - 1960)},
  Year                     = {1956},
  Number                   = {3/4},
  Pages                    = {221-229},
  Volume                   = {16},

  Owner                    = {iurteaga},
  Publisher                = {Springer},
  Timestamp                = {2017.05.10}
}

@Book{b-Bernardo2009,
  Title                    = {{Bayesian Theory}},
  Author                   = {Jos\'{e} M. Bernardo and Adrian F.M. Smith},
  Publisher                = {Wiley},
  Year                     = {2009},
  Series                   = {Wiley Series in Probability and Statistics},

  Doi                      = {10.1002/9780470316870},
  ISBN                     = {9780470317716},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Beygelzimer2008,
  Title                    = {{The Offset Tree for Learning with Partial Labels}},
  Author                   = {Alina Beygelzimer and John Langford},
  Journal                  = {CoRR},
  Year                     = {2008},
  Volume                   = {abs/0812.4044},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Book{b-Bishop2006,
  Title                    = {{Pattern Recognition and Machine Learning}},
  Author                   = {Christopher Bishop},
  Publisher                = {Springer-Verlag New York},
  Year                     = {2006},
  Series                   = {Information Science and Statistics},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.13}
}

@Article{j-Blei2012,
  Title                    = {{Probabilistic Topic Models}},
  Author                   = {David M. Blei},
  Journal                  = {Commun. ACM},
  Year                     = {2012},

  Month                    = apr,
  Number                   = {4},
  Pages                    = {77--84},
  Volume                   = {55},

  Acmid                    = {2133826},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/2133806.2133826},
  ISSN                     = {0001-0782},
  Issue_date               = {April 2012},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Publisher                = {ACM},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2006a,
  Title                    = {{Variational inference for Dirichlet process mixtures}},
  Author                   = {David M. Blei and Michael I. Jordan},
  Journal                  = {Bayesian analysis},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {121--144},
  Volume                   = {1},

  Abstract                 = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blei2004,
  Title                    = {{Variational Methods for the Dirichlet Process}},
  Author                   = {David M. Blei and Michael I. Jordan},
  Booktitle                = {Proceedings of the Twenty-first International Conference on Machine Learning},
  Year                     = {2004},

  Address                  = {New York, NY, USA},
  Pages                    = {12--},
  Publisher                = {ACM},
  Series                   = {ICML '04},

  Acmid                    = {1015439},
  Doi                      = {10.1145/1015330.1015439},
  ISBN                     = {1-58113-838-5},
  Location                 = {Banff, Alberta, Canada},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blei2006,
  Title                    = {{Dynamic topic models}},
  Author                   = {David M. Blei and John D. Lafferty},
  Booktitle                = {Proceedings of the 23rd international conference on Machine learning},
  Year                     = {2006},
  Organization             = {ACM},
  Pages                    = {113--120},

  Abstract                 = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR’ed archives of the journal Science from 1880 through 2000.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2006,
  Title                    = {{Correlated topic models}},
  Author                   = {David M. Blei and John D. Lafferty},
  Journal                  = {Advances in neural information processing systems},
  Year                     = {2006},
  Pages                    = {147},
  Volume                   = {18},

  Owner                    = {iurteaga},
  Publisher                = {MIT; 1998},
  Timestamp                = {2016.11.15}
}

@Article{j-Blei2003,
  Title                    = {{Latent Dirichlet allocation}},
  Author                   = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  Journal                  = {Journal of machine Learning research},
  Year                     = {2003},
  Number                   = {Jan},
  Pages                    = {993--1022},
  Volume                   = {3},

  Abstract                 = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Blundell2015,
  Title                    = {{Weight Uncertainty in Neural Networks}},
  Author                   = {Charles Blundell and Julien Cornebise and Koray Kavukcuoglu and Daan Wierstra},
  Booktitle                = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
  Year                     = {2015},
  Pages                    = {1613--1622},
  Publisher                = {JMLR.org},
  Series                   = {ICML'15},

  Acmid                    = {3045290},
  Location                 = {Lille, France},
  Numpages                 = {10},
  Owner                    = {iurteaga},
  Timestamp                = {2017.11.28}
}

@Article{j-Bottou2012,
  Title                    = {{Counterfactual Reasoning and Learning Systems}},
  Author                   = {L{\'{e}}on Bottou and Jonas Peters and Joaquin Qui{\~{n}}onero Candela and Denis Xavier Charles and Max Chickering and Elon Portugaly and Dipankar Ray and Patrice Y. Simard and Ed Snelson},
  Journal                  = {CoRR},
  Year                     = {2012},
  Volume                   = {abs/1209.2355},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Article{j-Brezzi2002,
  Title                    = {{Optimal learning and experimentation in bandit problems}},
  Author                   = {Monica Brezzi and Tze Leung Lai},
  Journal                  = {Journal of Economic Dynamics and Control},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {87 - 108},
  Volume                   = {27},

  Doi                      = {https://doi.org/10.1016/S0165-1889(01)00028-8},
  ISSN                     = {0165-1889},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Brezzi2000,
  Title                    = {{Incomplete Learning from Endogenous Data in Dynamic Allocation}},
  Author                   = {Monica Brezzi and Tze Leung Lai},
  Journal                  = {Econometrica},
  Year                     = {2000},
  Number                   = {6},
  Pages                    = {1511--1516},
  Volume                   = {68},

  Doi                      = {10.1111/1468-0262.00170},
  ISSN                     = {1468 0262},
  Owner                    = {iurteaga},
  Publisher                = {Blackwell Publishers Ltd},
  Timestamp                = {2017.05.10}
}

@Article{j-Burnetas1997,
  author    = {Apostolos N. Burnetas and Michael N. Katehakis},
  journal   = {Mathematics of Operations Research},
  title     = {{Optimal Adaptive Policies for Markov Decision Processes}},
  year      = {1997},
  number    = {1},
  pages     = {222-255},
  volume    = {22},
  abstract  = {In this paper we consider the problem of adaptive control for Markov Decision Processes. We give the explicit form for a class of adaptive policies that possess optimal increase rate properties for the total expected finite horizon reward, under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of the proposed policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations.},
  doi       = {10.1287/moor.22.1.222},
  owner     = {iurteaga},
  timestamp = {2018.07.30},
}

@Article{j-Carvalho2010,
  Title                    = {{Particle Learning and Smoothing}},
  Author                   = {Carlos M Carvalho and Michael S. Johannes and Hedibert F. Lopes and Nicholas G. Polson},
  Journal                  = {Statist. Sci.},
  Year                     = {2010},

  Month                    = {02},
  Number                   = {1},
  Pages                    = {88--106},
  Volume                   = {25},

  Fjournal                 = {Statistical Science},
  Owner                    = {iurteaga},
  Publisher                = {The Institute of Mathematical Statistics},
  Timestamp                = {2017-09-30}
}

@Article{j-Cesa-Bianchi2011,
  Title                    = {{An Optimal Algorithm for Linear Bandits}},
  Author                   = {Nicol\`o Cesa-Bianchi and Sham Kakade},
  Journal                  = {ArXiv e-prints},
  Year                     = {2011},

  Month                    = oct,

  Archiveprefix            = {arXiv},
  Eprint                   = {1110.4322},
  Keywords                 = {Computer Science - Learning, Statistics - Machine Learning},
  Owner                    = {iurteaga},
  Primaryclass             = {cs.LG},
  Timestamp                = {2018.07.30}
}

@InProceedings{ip-Chang2009,
  Title                    = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
  Author                   = {Jonathan Chang and Jordan Boyd-Graber and Chong Wang and Sean Gerrish and David M. Blei},
  Booktitle                = {Neural Information Processing Systems},
  Year                     = {2009},

  Keywords                 = {Topic models, interpretation, held-out likelihood},
  Location                 = {Vancouver, BC},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@InCollection{ic-Chapelle2011,
  author    = {Olivier Chapelle and Lihong Li},
  title     = {{An Empirical Evaluation of Thompson Sampling}},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {Curran Associates, Inc.},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {2249--2257},
  owner     = {iurteaga},
  timestamp = {2016.09.19},
  url       = {https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling},
}

@Article{j-Chopin2004,
  Title                    = {{Central Limit Theorem for Sequential Monte Carlo Methods and Its Application to Bayesian Inference}},
  Author                   = {Nicolas Chopin},
  Journal                  = {The Annals of Statistics},
  Year                     = {2004},
  Number                   = {6},
  Pages                    = {2385-2411},
  Volume                   = {32},

  ISSN                     = {00905364},
  Owner                    = {iurteaga},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2018.02.06}
}

@Article{j-Chopin2011,
  author        = {Nicolas Chopin and Pierre E. Jacob and Omiros Papaspiliopoulos},
  journal       = {arXiv preprint arXiv:1101.1528},
  title         = {{SMC$^2$: an efficient algorithm for sequential analysis of state-space models}},
  year          = {2011},
  month         = jan,
  archiveprefix = {arXiv},
  eprint        = {1101.1528},
  keywords      = {Statistics - Computation, 62F15, 65C05},
  owner         = {iurteaga},
  primaryclass  = {stat.CO},
  timestamp     = {2017-09-30},
  url           = {https://arxiv.org/abs/1101.1528},
}

@InProceedings{ip-Chu2011,
  Title                    = {{Contextual Bandits with Linear Payoff Functions}},
  Author                   = {Wei Chu and Lihong Li and Lev Reyzin and Robert Schapire},
  Booktitle                = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2011},

  Address                  = {Fort Lauderdale, FL, USA},
  Editor                   = {Geoffrey Gordon and David Dunson and Miroslav Dud\'ik},
  Month                    = {11--13 Apr},
  Pages                    = {208--214},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {15},

  Abstract                 = {In this paper we study the contextual bandit problem (also known as the multi-armed bandit problem with expert advice) for linear payoff functions. For T rounds, K actions, and d dimensional feature vectors, we prove an $O(\sqrt{Td\ln^3(KT\ln(T)/\delta}))$ regret bound that holds with probability $1-\delta$ for the simplest known (both conceptually and computationally) efficient upper confidence bound algorithm for this problem. We also prove a lower bound of \Omega(\sqrt{Td})$ for this setting, matching the upper bound up to logarithmic factors.},
  Url                      = {http://proceedings.mlr.press/v15/chu11a.html}
}

@InProceedings{ip-Chu2009,
  Title                    = {{A Case Study of Behavior-driven Conjoint Analysis on Yahoo!: Front Page Today Module}},
  Author                   = {Chu, Wei and Park, Seung-Taek and Beaupre, Todd and Motgi, Nitin and Phadke, Amit and Chakraborty, Seinjuti and Zachariah, Joe},
  Booktitle                = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {1097--1104},
  Publisher                = {ACM},
  Series                   = {KDD '09},

  Acmid                    = {1557138},
  Doi                      = {10.1145/1557019.1557138},
  ISBN                     = {978-1-60558-495-9},
  Keywords                 = {classification, clustering, conjoint analysis, logistic regression, segmentation, tensor product},
  Location                 = {Paris, France},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Article{j-Creal2012,
  author    = {Drew Creal},
  journal   = {Econometric Reviews},
  title     = {{A Survey of Sequential Monte Carlo Methods for Economics and Finance}},
  year      = {2012},
  number    = {3},
  pages     = {245-296},
  volume    = {31},
  abstract  = {This article serves as an introduction and survey for economists to the field of sequential Monte Carlo methods which are also known as particle filters. Sequential Monte Carlo methods are simulation-based algorithms used to compute the high-dimensional and/or complex integrals that arise regularly in applied work. These methods are becoming increasingly popular in economics and finance; from dynamic stochastic general equilibrium models in macro-economics to option pricing. The objective of this article is to explain the basics of the methodology, provide references to the literature, and cover some of the theoretical results that justify the methods in practice.},
  keywords  = {Kalman filter, Markov chain Monte Carlo, Particle filter, Sequential Monte Carlo, State space models, Finance, Economics},
  owner     = {iurteaga},
  timestamp = {2013-11-14},
}

@Article{j-Crisan2002,
  Title                    = {{A survey of convergence results on particle filtering methods for practitioners}},
  Author                   = {Dan Crisan and Arnaud Doucet},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2002},

  Month                    = {Mar},
  Number                   = {3},
  Pages                    = {736-746},
  Volume                   = {50},

  Abstract                 = {Optimal filtering problems are ubiquitous in signal processing and related fields. Except for a restricted class of models, the optimal filter does not admit a closed-form expression. Particle filtering methods are a set of flexible and powerful sequential Monte Carlo methods designed to. solve the optimal filtering problem numerically. The posterior distribution of the state is approximated by a large set of Dirac-delta masses (samples/particles) that evolve randomly in time according to the dynamics of the model and the observations. The particles are interacting; thus, classical limit theorems relying on statistically independent samples do not apply. In this paper, our aim is to present a survey of convergence results on this class of methods to make them accessible to practitioners},
  Doi                      = {10.1109/78.984773},
  ISSN                     = {1053-587X},
  Keywords                 = {Monte Carlo methods;convergence of numerical methods;filtering theory;optimisation;signal processing;Dirac-delta masses;closed-form expression;convergence results survey;general state-space models;model dynamics;optimal filter;optimal filtering;particle filtering methods;posterior distribution;recursive algorithm;samples/particles;sequential Monte Carlo methods;signal processing;statistically independent samples;Bayesian methods;Closed-form solution;Convergence;Design methodology;Filtering algorithms;Filters;Hidden Markov models;Monte Carlo methods;Signal processing;State estimation},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@Article{j-Crisan2013,
  author        = {Dan Crisan and Joaqu\'{i}n M\'{i}guez},
  journal       = {arXiv preprint arXiv:1308.1883},
  title         = {{Nested particle filters for online parameter estimation in discrete-time state-space Markov models}},
  year          = {2013},
  month         = {Aug},
  archiveprefix = {arXiv},
  eprint        = {1308.1883},
  keywords      = {Statistics - Computation, Mathematics - Numerical Analysis, Mathematics - Probability, 60J05, 60F05, 65C05},
  owner         = {iurteaga},
  primaryclass  = {stat.CO},
  timestamp     = {2017-09-30},
  url           = {https://arxiv.org/abs/1308.1883},
}

@Article{j-Dempster1977,
  Title                    = {{Maximum likelihood from incomplete data via the EM algorithm}},
  Author                   = {Arthur P. Dempster and Nan M. Laird and Donald B. Rubin},
  Journal                  = {Journal of the royal statistical society. Series B (methodological)},
  Year                     = {1977},
  Pages                    = {1--38},

  Owner                    = {iurteaga},
  Publisher                = {Royal Statistical Society},
  Timestamp                = {2017.05.19}
}

@InBook{ib-Djuric2010,
  Title                    = {{Particle Filtering}},
  Author                   = {Petar M. Djuri\'{c} and M\'{o}nica F. Bugallo},
  Chapter                  = {5},
  Pages                    = {271-331},
  Publisher                = {Wiley-Blackwell},
  Year                     = {2010},

  Abstract                 = {Summary This chapter contains sections titled: Introduction Motivation for Use of Particle Filtering The Basic Idea The Choice of Proposal Distribution and Resampling Some Particle Filtering Methods Handling Constant Parameters Rao Blackwellization Prediction Smoothing Convergence Issues Computational Issues and Hardware Implementation Acknowledgments Exercises References},
  Booktitle                = {Adaptive Signal Processing},
  Doi                      = {10.1002/9780470575758.ch5},
  Eprint                   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470575758.ch5},
  ISBN                     = {9780470575758},
  Keywords                 = {motivation for use of particle filtering, choice of proposal distribution and resampling, Rao Blackwellization, variance of estimate reduction, and Monte Carlo sampling methods},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@InProceedings{ip-Djuric2004,
  Title                    = {{Density assisted particle filters for state and parameter estimation}},
  Author                   = {Petar M. Djuri\'{c} and M\'{o}nica F. Bugallo and Joaqu\'{i}n M\'{i}guez},
  Booktitle                = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, (ICASSP)},
  Year                     = {2004},
  Month                    = {5},
  Pages                    = {ii - 701-704},
  Volume                   = {2},

  Abstract                 = {In recent years the theory of particle filtering has continued to advance, and it has found increasing use in sequential signal processing. A weakness of particle filtering is that it is inadequate for problems that besides tracking of evolving states require the estimation of constant parameters. In this paper, we propose particle filters that do not have this limitation. We call these filters density assisted particle filters, of which special cases are the recently introduced Gaussian particle filters and Gaussian sum particle filters. An implementation of a density particle filter is shown on a relatively simple but important nonlinear model. Simulations are included that show the performance of this filter.},
  Doi                      = {10.1109/ICASSP.2004.1326354},
  ISSN                     = {1520-6149},
  Keywords                 = {Gaussian particle filters; Gaussian sum particle filters; density assisted particle filters; nonlinear model; parameter estimation; particle filtering; performance; sequential signal processing; state estimation; Gaussian distribution; filtering theory; nonlinear filters; parameter estimation; sequential estimation; signal sampling; state estimation;},
  Owner                    = {iurteaga},
  Timestamp                = {2012-08-29}
}

@Article{j-Djuric2003,
  Title                    = {{Particle Filtering}},
  Author                   = {Petar M. Djuri\'{c} and Jayesh H. Kotecha and Jianqui Zhang and Yufei Huang and Tadesse Ghirmai and M\'{o}nica F. Bugallo and Joaqu\'{i}n M\'{i}guez},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2003},

  Month                    = {9},
  Pages                    = {19-38},
  Volume                   = {20(5)},

  Abstract                 = {Recent developments have demonstrated that particle filtering is an emerging and powerful methodology for sequential signal processing with a wide range of applications in science and engineering. It has captured the attention of many researchers in various communities including those of signal processing, statistics, and econometrics, and this interest stems from its potential for coping with difficult nonlinear and/or non-Gaussian problems. Based on the concept of sequential importance sampling and the use of Bayesian theory, particle filtering is particularly useful in dealing with nonlinear and non-Gaussian problems. The underlying principle of the methodology is the approximation of relevant distributions with random measures composed of particles (samples from the space of the unknowns) and their associated weights. In this article, first we present a brief review of the particle filtering theory, and then we show how it can be used for resolving many problems in wireless communications. We demonstrate its application to blind equalization, blind detection over flat fading channels, multiuser detection, and estimation and detection of space-time codes in fading channels.},
  Keywords                 = {Particle Filtering, review, wireleless communications},
  Owner                    = {iurteaga},
  Timestamp                = {2012-05-31}
}

@InProceedings{ip-Doucet2000,
  Title                    = {{Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks}},
  Author                   = {Arnaud Doucet and Nando de Freitas and Kevin P. Murphy and Stuart J. Russell},
  Booktitle                = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
  Year                     = {2000},

  Address                  = {San Francisco, CA, USA},
  Pages                    = {176--183},
  Publisher                = {Morgan Kaufmann Publishers Inc.},
  Series                   = {UAI '00},

  Acmid                    = {720075},
  ISBN                     = {1-55860-709-9},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Article{j-Gershman2012,
  Title                    = {{A tutorial on Bayesian nonparametric models}},
  Author                   = {Samuel J. Gershman and David M. Blei},
  Journal                  = {Journal of Mathematical Psychology},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1 - 12},
  Volume                   = {56},

  Doi                      = {https://doi.org/10.1016/j.jmp.2011.08.004},
  ISSN                     = {0022-2496},
  Keywords                 = {Bayesian methods, Chinese restaurant process, Indian buffet process},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Ghavamzadeh2015,
  Title                    = {{Bayesian Reinforcement Learning: A Survey}},
  Author                   = {Mohammad Ghavamzadeh and Shie Mannor and Joelle Pineau and Aviv Tamar},
  Journal                  = {Foundations and Trends® in Machine Learning},
  Year                     = {2015},
  Number                   = {5-6},
  Pages                    = {359-483},
  Volume                   = {8},

  Abstract                 = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/ exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  Doi                      = {10.1561/2200000049},
  ISSN                     = {1935-8237},
  Keywords                 = {Reinforcement Learning, Bayesian},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Article{j-Gittins1979,
  Title                    = {{Bandit Processes and Dynamic Allocation Indices}},
  Author                   = {J. C. Gittins},
  Journal                  = {Journal of the Royal Statistical Society. Series B (Methodological)},
  Year                     = {1979},
  Number                   = {2},
  Pages                    = {148-177},
  Volume                   = {41},

  Abstract                 = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
  ISSN                     = {00359246},
  Owner                    = {iurteaga},
  Publisher                = {[Royal Statistical Society, Wiley]},
  Timestamp                = {2016.09.20}
}

@InProceedings{ip-Gopalan2015,
  Title                    = {{Thompson Sampling for Learning Parameterized Markov Decision Processes}},
  Author                   = {Aditya Gopalan and Shie Mannor},
  Booktitle                = {Proceedings of The 28th Conference on Learning Theory},
  Year                     = {2015},

  Address                  = {Paris, France},
  Editor                   = {Peter Gr\"unwald and Elad Hazan and Satyen Kale},
  Month                    = {03--06 Jul},
  Pages                    = {861--898},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {40},

  Abstract                 = {We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space.},
  File                     = {Gopalan15.pdf:http\://proceedings.mlr.press/v40/Gopalan15.pdf:PDF},
  Url                      = {http://proceedings.mlr.press/v40/Gopalan15.html}
}

@Article{j-Gordon1993,
  Title                    = {{Novel approach to nonlinear/non-Gaussian Bayesian state estimation}},
  Author                   = {Neil J. Gordon and D.J. Salmond and A.F.M. Smith},
  Journal                  = {Radar and Signal Processing, IEEE Proceedings},
  Year                     = {1993},

  Month                    = {4},
  Number                   = {2},
  Pages                    = {107 -113},
  Volume                   = {140},

  Abstract                 = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter},
  ISSN                     = {0956-375X},
  Keywords                 = {Gaussian noise;algorithm;bearings only tracking problem;bootstrap filter;extended Kalman filter;measurement model;nonGaussian Bayesian state estimation;nonlinear Bayesian state estimation;random samples;recursive Bayesian filters;simulation;state transition model;state vector density;Bayes methods;Kalman filters;filtering and prediction theory;state estimation;tracking;},
  Owner                    = {iurteaga},
  Timestamp                = {2012-07-27}
}

@Article{j-Gosavi2009,
  Title                    = {{Reinforcement learning: A tutorial survey and recent advances}},
  Author                   = {Abhijit Gosavi},
  Journal                  = {INFORMS Journal on Computing},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {178--192},
  Volume                   = {21},

  Publisher                = {INFORMS}
}

@InProceedings{ip-Gruenewaelder2010,
  Title                    = {{Regret Bounds for Gaussian Process Bandit Problems}},
  Author                   = {Steffen Gr\"unew\"alder and Jean--Yves Audibert and Manfred Opper and John Shawe--Taylor},
  Booktitle                = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2010},

  Address                  = {Chia Laguna Resort, Sardinia, Italy},
  Editor                   = {Yee Whye Teh and Mike Titterington},
  Month                    = {13--15 May},
  Pages                    = {273--280},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {9},

  Abstract                 = {Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modeled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function defining the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds.},
  Url                      = {http://proceedings.mlr.press/v9/grunewalder10a.html}
}

@Article{j-Hoffman2013,
  Title                    = {{Stochastic variational inference}},
  Author                   = {Matthew D. Hoffman and David M. Blei and Chong Wang and John William Paisley},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2013},

  Abstract                 = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions.
We develop this technique for a large class of probabilistic models and we demonstrate
it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process
topic model. Using stochastic variational inference, we analyze several large collections of
documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles
from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms
traditional variational inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational
inference lets us apply complex Bayesian models to massive data sets.},
  Keywords                 = {Bayesian inference, variational inference, stochastic optimization, topic models, Bayesian nonparametrics},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Article{j-Ionides2006,
  Title                    = {{Inference for nonlinear dynamical systems}},
  Author                   = {Edward L. Ionides and C. Bret\'{o} and A. A. King},
  Journal                  = {Proceedings of the National Academy of Sciences},
  Year                     = {2006},
  Number                   = {49},
  Pages                    = {18438-18443},
  Volume                   = {103},

  Abstract                 = {Nonlinear stochastic dynamical systems are widely used to model systems across the sciences and engineering. Such models are natural to formulate and can be analyzed mathematically and numerically. However, difficulties associated with inference from time-series data about unknown parameters in these models have been a constraint on their application. We present a new method that makes maximum likelihood estimation feasible for partially-observed nonlinear stochastic dynamical systems (also known as state-space models) where this was not previously the case. The method is based on a sequence of filtering operations which are shown to converge to a maximum likelihood parameter estimate. We make use of recent advances in nonlinear filtering in the implementation of the algorithm. We apply the method to the study of cholera in Bangladesh. We construct confidence intervals, perform residual analysis, and apply other diagnostics. Our analysis, based upon a model capturing the intrinsic nonlinear dynamics of the system, reveals some effects overlooked by previous studies.},
  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@Article{j-Kalman1960,
  author    = {Rudolph Emil Kalman},
  title     = {{A New Approach to Linear Filtering and Prediction Problems}},
  journal   = {Transactions of the ASME--Journal of Basic Engineering},
  year      = {1960},
  volume    = {82},
  number    = {Series D},
  pages     = {35-45},
  owner     = {iurteaga},
  timestamp = {2014-11-10},
}

@InProceedings{ip-Kaufmann2012,
  Title                    = {{On Bayesian Upper Confidence Bounds for Bandit Problems}},
  Author                   = {Emilie Kaufmann and Olivier Cappe and Aurelien Garivier},
  Booktitle                = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2012},

  Address                  = {La Palma, Canary Islands},
  Editor                   = {Neil D. Lawrence and Mark Girolami},
  Month                    = {21--23 Apr},
  Pages                    = {592--600},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {22},

  Abstract                 = {Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed Bayes-UCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.},
  Owner                    = {iurteaga},
  Timestamp                = {2017.10.11}
}

@Misc{Kearns1999,
  Title                    = {{Approximate Planning in Large POMDPs via Reusable Trajectories }},

  Author                   = {Michael Kearns and Yishay Mansour and Andrew Y. Ng},
  Year                     = {1999},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Kingma2015,
  Title                    = {{Variational Dropout and the Local Reparameterization Trick}},
  Author                   = {Diederik P Kingma and Tim Salimans and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems 28},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2015},
  Editor                   = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  Pages                    = {2575--2583},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InCollection{ic-Korda2013,
  Title                    = {{Thompson Sampling for 1-Dimensional Exponential Family Bandits}},
  Author                   = {Nathaniel Korda and Emilie Kaufmann and R{\'e}mi Munos},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2013},
  Editor                   = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  Pages                    = {1448--1456},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.19}
}

@InCollection{ic-Krause2011,
  Title                    = {{Contextual Gaussian Process Bandit Optimization}},
  Author                   = {Andreas Krause and Cheng S. Ong},
  Booktitle                = {Advances in Neural Information Processing Systems 24},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  Pages                    = {2447--2455},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Lai1987,
  Title                    = {{Adaptive Treatment Allocation and the Multi-Armed Bandit Problem}},
  Author                   = {Tze Leung Lai},
  Journal                  = {The Annals of Statistics},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {1091-1114},
  Volume                   = {15},

  ISSN                     = {00905364},
  Owner                    = {iurteaga},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2017.05.10}
}

@Article{j-Lai1985,
  Title                    = {{Asymptotically Efficient Adaptive Allocation Rules}},
  Author                   = {Tze Leung Lai and Herbert Robbins},
  Journal                  = {Advances in Applied Mathematics},
  Year                     = {1985},

  Month                    = {mar},
  Number                   = {1},
  Pages                    = {4--22},
  Volume                   = {6},

  Acmid                    = {2609757},
  Address                  = {Orlando, FL, USA},
  Doi                      = {10.1016/0196-8858(85)90002-8},
  ISSN                     = {0196-8858},
  Issue_date               = {March, 1985},
  Numpages                 = {19},
  Owner                    = {iurteaga},
  Publisher                = {Academic Press, Inc.},
  Timestamp                = {2017.05.10}
}

@InProceedings{ip-Lamprier2017,
  Title                    = {{Variational Thompson Sampling for Relational Recurrent Bandits}},
  Author                   = {Sylvain Lamprier and Thibault Gisselbrecht and Patrick Gallinari},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Year                     = {2017},

  Address                  = {Cham},
  Editor                   = {Michelangelo Ceci and Jaakko Hollm{\'e}n and Ljup{\v{c}}o Todorovski and Celine Vens and Sa{\v{s}}o D{\v{z}}eroski},
  Pages                    = {405--421},
  Publisher                = {Springer International Publishing},

  Abstract                 = {In this paper, we introduce a novel non-stationary bandit setting, called relational recurrent bandit, where rewards of arms at successive time steps are interdependent. The aim is to discover temporal and structural dependencies between arms in order to maximize the cumulative collected reward. Two algorithms are proposed: the first one directly models temporal dependencies between arms, as the second one assumes the existence of hidden states of the system behind the observed rewards. For both approaches, we develop a Variational Thompson Sampling method, which approximates distributions via variational inference, and uses the estimated distributions to sample reward expectations at each iteration of the process. Experiments conducted on both synthetic and real data demonstrate the effectiveness of our approaches.},
  ISBN                     = {978-3-319-71246-8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@InProceedings{ip-Langford2005,
  Title                    = {{Relating Reinforcement Learning Performance to Classification Performance}},
  Author                   = {John Langford and Bianca Zadrozny},
  Booktitle                = {Proceedings of the 22Nd International Conference on Machine Learning},
  Year                     = {2005},

  Address                  = {New York, NY, USA},
  Pages                    = {473--480},
  Publisher                = {ACM},
  Series                   = {ICML '05},

  Acmid                    = {1102411},
  Doi                      = {10.1145/1102351.1102411},
  ISBN                     = {1-59593-180-5},
  Location                 = {Bonn, Germany},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Langford2008,
  Title                    = {{The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information}},
  Author                   = {John Langford and Tong Zhang},
  Booktitle                = {Advances in Neural Information Processing Systems 20},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2008},
  Editor                   = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
  Pages                    = {817--824},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {https://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information}
}

@Article{j-Leeuwen2009,
  Title                    = {{Particle Filtering in Geophysical Systems}},
  Author                   = {Peter Jan van Leeuwen},
  Journal                  = {Monthly Weather Review},
  Year                     = {2009},
  Number                   = {137},
  Pages                    = {4089-4114.},
  Volume                   = {12},

  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@InProceedings{ip-Li2016,
  Title                    = {{Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks}},
  Author                   = {Chunyuan Li and Changyou Chen and David Carlson and Lawrence Carin},
  Booktitle                = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  Year                     = {2016},
  Pages                    = {1788--1794},
  Publisher                = {AAAI Press},
  Series                   = {AAAI'16},

  Acmid                    = {3016149},
  Location                 = {Phoenix, Arizona},
  Numpages                 = {7},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Li2013,
  author        = {Lihong Li},
  journal       = {arXiv preprint arXiv:1310.7163},
  title         = {{Generalized Thompson Sampling for Contextual Bandits}},
  year          = {2013},
  month         = oct,
  archiveprefix = {arXiv},
  eprint        = {1310.7163},
  keywords      = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Other Statistics, 62L05, I.2.6},
  owner         = {iurteaga},
  primaryclass  = {cs.LG},
  timestamp     = {2016.09.19},
  url           = {https://arxiv.org/abs/1310.7163},
}

@Article{j-Li2015,
  Title                    = {{Resampling Methods for Particle Filtering: Classification, implementation, and strategies}},
  Author                   = {Tiancheng Li and Miodrag Boli\'{c} and Petar M. Djuri\'{c}},
  Journal                  = {Signal Processing Magazine, IEEE},
  Year                     = {2015},

  Month                    = {5},
  Number                   = {3},
  Pages                    = {70-86},
  Volume                   = {32},

  Abstract                 = {Two decades ago, with the publication, we witnessed the rebirth of particle filtering (PF) as a methodology for sequential signal processing. Since then, PF has become very popular because of its ability to process observations represented by nonlinear state-space models where the noises of the model can be non-Gaussian. This methodology has been adopted in various fields, including finance, geophysical systems, wireless communications, control, navigation and tracking, and robotics. The popularity of PF has also spurred the publication of several review articles. In this article, the state of the art of resampling methods was reviewed. The methods were classified and their properties were compared in the framework of the proposed classifications. The emphasis in the article was on the classification and qualitative descriptions of the algorithms. The intention was to provide guidelines to practitioners and researchers.},
  ISSN                     = {1053-5888},
  Keywords                 = {mobile robots;particle filtering (numerical methods);radio tracking;radionavigation;signal classification;state-space methods;telecommunication control;finance;geophysical systems;navigation;nonGaussian noise;nonlinear state-space models;particle classification;particle filtering;resampling methods;robotics;sequential signal processing;tracking;wireless communications;wireless control;Approximation algorithms;Approximation methods;Atmospheric measurements;Filtering;Particle measurements;Signal processing algorithms;Systematics},
  Owner                    = {iurteaga},
  Timestamp                = {2015-08-11}
}

@InBook{ib-Liu2001,
  Title                    = {{Combined Parameter and State Estimation in Simulation-Based Filtering}},
  Author                   = {Jane Liu and Mike West},
  Chapter                  = {10},
  Editor                   = {Arnaud Doucet and Nando de Freitas and Neil Gordon},
  Pages                    = {197--223},
  Publisher                = {Springer New York},
  Year                     = {2001},

  Address                  = {New York, NY},

  Abstract                 = {Much of the recent and current interest in simulation-based methods of sequential Bayesian analysis of dynamic models has been focused on improved methods of filtering for time-varying state vectors. We now have quite effective algorithms for time-varying states, as represented throughout this volume. Variants of the auxiliary particle filtering algorithm (Pitt and Shephard 1999b), in particular, are of proven applied efficacy in quite elaborate models. However, the need for more general algorithms that deal simultaneously with both fixed model parameters and state variables is especially pressing. We simply do not have access to efficient and effective methods of treating this problem, especially in models with realistically large numbers of fixed model parameters. It is a very challenging problem.},
  Booktitle                = {Sequential Monte Carlo Methods in Practice},
  Doi                      = {10.1007/978-1-4757-3437-9_10},
  ISBN                     = {978-1-4757-3437-9},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.14}
}

@Book{b-Liu2001,
  Title                    = {{Monte Carlo Strategies in Scientific Computing}},
  Author                   = {Jun S. Liu},
  Publisher                = {Springer},
  Year                     = {2001},
  Series                   = {Springer Series in Statistics},

  Abstract                 = {This book provides a self-contained and up-to-date treatment of the Monte Carlo method and develops a common framework under which various Monte Carlo techniques can be "standardized" and compared. Given the interdisciplinary nature of the topics and a moderate prerequisite for the reader, this book should be of interest to a broad audience of quantitative researchers such as computational biologists, computer scientists, econometricians, engineers, probabilists, and statisticians. It can also be used as the textbook for a graduate-level course on Monte Carlo methods. Many problems discussed in the alter chapters can be potential thesis topics for masters or Ph.D. students in statistics or computer science departments.},
  Keywords                 = {Monte Carlo method; Sequential Monte-Carlo; Metropolis Algorithm; Gibbs Sampler; Conditional Sampling; Population-Based Monte Carlo Methods},
  Owner                    = {iurteaga},
  Pages                    = {346},
  Timestamp                = {2012-08-20}
}

@InProceedings{ip-Maillard2011,
  Title                    = {{Finite-Time Analysis of Multi-armed Bandits Problems with Kullback-Leibler Divergences}},
  Author                   = {Odalric-Ambrym Maillard and R{\'e}mi Munos and Gilles Stoltz},
  Booktitle                = {Conference On Learning Theory},
  Year                     = {2011},

  Abstract                 = {We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed bandit problem in the case of distributions with finite supports (not necessarily known beforehand), whose asymptotic regret matches the lower bound of Burnetas and Katehakis (1996). Our contribution is to provide a finite-time analysis of this algorithm; we get bounds whose main terms are smaller than the ones of previously known algorithms with finite-time analyses (like UCB-type algorithms)},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.19}
}

@Article{j-Martino2017,
  author    = {Luca Martino and V\'{i}ctor Elvira and Francisco Louzada},
  journal   = {Signal Processing},
  title     = {{Effective sample size for importance sampling based on discrepancy measures}},
  year      = {2017},
  issn      = {0165-1684},
  pages     = {386 - 401},
  volume    = {131},
  abstract  = {Abstract The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the \{IS\} context, an approximation \{ESS\} ^ of the theoretical \{ESS\} definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, \{ESS\} ^ , has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression \{ESS\} ^ is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible \{ESS\} functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic \{ESS\} function should satisfy, allowing us to classify different \{ESS\} measures. We also compare the most promising ones by means of numerical simulations.},
  keywords  = {Effective Sample Size},
  owner     = {iurteaga},
  timestamp = {2017.07.09},
}

@Book{b-McLachlan2004,
  Title                    = {{Finite Mixture Models}},
  Author                   = {Geoffrey McLachlan and David Peel},
  Publisher                = {John Wiley \& Sons, 2004},
  Year                     = {2004},
  Series                   = {Wiley Series in Probability and Statistics},

  ISBN                     = {9780471654063},
  Owner                    = {iurteaga},
  Timestamp                = {2017.05.12}
}

@Article{j-Neal2000,
  Title                    = {{Markov Chain Sampling Methods for Dirichlet Process Mixture Models}},
  Author                   = {Radford M. Neal},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2000},
  Number                   = {2},
  Pages                    = {249--265},
  Volume                   = {9},

  Abstract                 = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
  ISSN                     = {10618600},
  Owner                    = {iurteaga},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  Timestamp                = {2018.05.16}
}

@Article{j-Olsson2006,
  author    = {{Olsson}, J. and {Capp{\'e}}, O. and {Douc}, R. and {Moulines}, E.},
  journal   = {arXiv preprint arXiv:math/0609514},
  title     = {{Sequential Monte Carlo smoothing with application to parameter estimation in non-linear state space models}},
  year      = {2006},
  month     = {Sep},
  eprint    = {math/0609514},
  keywords  = {Mathematics - Statistics},
  owner     = {iurteaga},
  timestamp = {2017.07.09},
  url       = {https://arxiv.org/abs/math/0609514},
}

@Article{j-Olsson2014,
  author        = {{Olsson}, J. and {Westerborn}, J.},
  journal       = {arXiv preprint arXiv:1412.7550},
  title         = {{Efficient particle-based online smoothing in general hidden Markov models: the PaRIS algorithm}},
  year          = {2014},
  month         = {Dec},
  archiveprefix = {arXiv},
  eprint        = {1412.7550},
  keywords      = {Statistics - Computation, 62M09 (Primary) 62F12 (Secondary)},
  owner         = {iurteaga},
  primaryclass  = {stat.CO},
  timestamp     = {2017.07.09},
  url           = {https://arxiv.org/abs/1412.7550},
}

@Article{j-Ortega2010,
  Title                    = {{A Minimum Relative Entropy Principle for Learning and Acting}},
  Author                   = {Pedro A. Ortega and Daniel A. Braun},
  Journal                  = {Journal of Artificial Intelligence Research},
  Year                     = {2010},

  Month                    = may,
  Number                   = {1},
  Pages                    = {475--511},
  Volume                   = {38},

  Acmid                    = {1892223},
  Address                  = {USA},
  ISSN                     = {1076-9757},
  Issue_date               = {May 2010},
  Numpages                 = {37},
  Owner                    = {iurteaga},
  Publisher                = {AI Access Foundation},
  Timestamp                = {2016.09.19}
}

@InCollection{ic-Osband2016,
  Title                    = {{Deep Exploration via Bootstrapped DQN}},
  Author                   = {Ian Osband and Charles Blundell and Alexander Pritzel and Benjamin Van Roy},
  Booktitle                = {Advances in Neural Information Processing Systems 29},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2016},
  Editor                   = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  Pages                    = {4026--4034},

  Owner                    = {iurteaga},
  Timestamp                = {2017.11.28}
}

@InCollection{ic-Ouyang2017,
  Title                    = {{Learning Unknown Markov Decision Processes: A Thompson Sampling Approach}},
  Author                   = {Yi Ouyang and Mukul Gagrani and Ashutosh Nayyar and Rahul Jain},
  Booktitle                = {Advances in Neural Information Processing Systems 30},
  Publisher                = {Curran Associates, Inc.},
  Year                     = {2017},
  Editor                   = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  Pages                    = {1333--1342},

  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {http://papers.nips.cc/paper/6732-learning-unknown-markov-decision-processes-a-thompson-sampling-approach.pdf}
}

@Article{j-Pearl2009,
  Title                    = {{Causal inference in statistics: An overview}},
  Author                   = {Judea Pearl},
  Journal                  = {Statist. Surv.},
  Year                     = {2009},
  Pages                    = {96--146},
  Volume                   = {3},

  Doi                      = {10.1214/09-SS057},
  Fjournal                 = {Statistics Surveys},
  Owner                    = {iurteaga},
  Publisher                = {The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada},
  Timestamp                = {2016.09.14}
}

@Article{j-Pivovarov2015,
  author    = {Rimma Pivovarov and Adler J. Perotte and Edouard Grave and John Angiolillo and Chris H. Wiggins and Noémie Elhadad},
  journal   = {Journal of Biomedical Informatics},
  title     = {{Learning probabilistic phenotypes from heterogeneous \{EHR\} data}},
  year      = {2015},
  issn      = {1532-0464},
  pages     = {156 - 165},
  volume    = {58},
  abstract  = {Abstract We present the Unsupervised Phenome Model (UPhenome), a probabilistic graphical model for large-scale discovery of computational models of disease, or phenotypes. We tackle this challenge through the joint modeling of a large set of diseases and a large set of clinical observations. The observations are drawn directly from heterogeneous patient record data (notes, laboratory tests, medications, and diagnosis codes), and the diseases are modeled in an unsupervised fashion. We apply \{UPhenome\} to two qualitatively different mixtures of patients and diseases: records of extremely sick patients in the intensive care unit with constant monitoring, and records of outpatients regularly followed by care providers over multiple years. We demonstrate that the \{UPhenome\} model can learn from these different care settings, without any additional adaptation. Our experiments show that (i) the learned phenotypes combine the heterogeneous data types more coherently than baseline LDA-based phenotypes; (ii) they each represent single diseases rather than a mix of diseases more often than the baseline ones; and (iii) when applied to unseen patient records, they are correlated with the patients’ ground-truth disorders. Code for training, inference, and quantitative evaluation is made available to the research community.},
  doi       = {http://dx.doi.org/10.1016/j.jbi.2015.10.001},
  keywords  = {Probabilistic modeling, Computational disease models, Phenotyping, Clinical phenotype modeling, Medical information systems, Electronic health record},
  owner     = {iurteaga},
  timestamp = {2016.11.15},
}

@Book{b-Rasmussen2005,
  Title                    = {{Gaussian Processes for Machine Learning}},
  Author                   = {Carl Edward Rasmussen and Christopher K. I. Williams},
  Publisher                = {The MIT Press},
  Year                     = {2005},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Riquelme2018,
  Title                    = {{Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling}},
  Author                   = {Carlos Riquelme and George Tucker and Jasper Snoek},
  Booktitle                = {International Conference on Learning Representations},
  Year                     = {2018},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.15}
}

@Book{b-Ristic2004,
  Title                    = {{Beyond the Kalman Filter: Particle Filters for Tracking Applications}},
  Author                   = {Branko Ristic and Sanjeev Arulampalam and Neil Gordon},
  Editor                   = {Artech Print},
  Publisher                = {Artech House},
  Year                     = {2004},

  ISBN                     = {9781580538510},
  Owner                    = {iurteaga},
  Timestamp                = {2014-11-10}
}

@Article{j-Robbins1956,
  Title                    = {{A sequential decision procedure with a finite memory}},
  Author                   = {Herbert Robbins},
  Journal                  = {Proceedings of the National Academy of Science},
  Year                     = {1956},
  Number                   = {42},
  Pages                    = {920 - 923},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Robbins1952,
  Title                    = {{Some aspects of the sequential design of experiments}},
  Author                   = {Herbert Robbins},
  Journal                  = {Bulletin of the American Mathematical Society},
  Year                     = {1952},
  Number                   = {58},
  Pages                    = {527-535},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Rusmevichientong2010,
  Title                    = {Linearly Parameterized Bandits},
  Author                   = {Paat Rusmevichientong and John N. Tsitsiklis},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {2010},

  Month                    = may,
  Number                   = {2},
  Pages                    = {395--411},
  Volume                   = {35},

  Acmid                    = {1836129},
  Address                  = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
  Doi                      = {10.1287/moor.1100.0446},
  ISSN                     = {0364-765X},
  Issue_date               = {May 2010},
  Keywords                 = {adaptive control, multi-armed bandit, parametric model},
  Numpages                 = {17},
  Owner                    = {iurteaga},
  Publisher                = {INFORMS},
  Timestamp                = {2018.07.30}
}

@Article{j-Russo2016,
  Title                    = {{An information-theoretic analysis of Thompson sampling}},
  Author                   = {Daniel Russo and Benjamin Van Roy},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {2442--2471},
  Volume                   = {17},

  Keywords                 = {Online optimization; Multi-armed bandits; Thompson sampling; Information theory; Regret bounds;},
  Owner                    = {iurteaga},
  Publisher                = {JMLR. org},
  Timestamp                = {2017.08.02}
}

@Article{j-Russo2014,
  Title                    = {{Learning to optimize via posterior sampling}},
  Author                   = {Daniel Russo and Benjamin Van Roy},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {2014},
  Number                   = {4},
  Pages                    = {1221--1243},
  Volume                   = {39},

  Keywords                 = {Online optimization; Multi-armed bandits; Thompson sampling; theoretical bounds;},
  Owner                    = {iurteaga},
  Timestamp                = {2017.08.02}
}

@Article{j-Russo2018,
  Title                    = {{A Tutorial on Thompson Sampling}},
  Author                   = {Daniel J. Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband and Zheng Wen},
  Journal                  = {{Foundations and Trends\textsuperscript{\textregistered} in Machine Learning}},
  Year                     = {2018},
  Number                   = {1},
  Pages                    = {1-96},
  Volume                   = {11},

  Doi                      = {10.1561/2200000070},
  ISSN                     = {1935-8237},
  Owner                    = {iurteaga},
  Timestamp                = {2018.07.30},
  Url                      = {http://dx.doi.org/10.1561/2200000070}
}

@Article{j-Scott2015,
  Title                    = {{Multi-armed bandit experiments in the online service economy}},
  Author                   = {Steven L. Scott},
  Journal                  = {Applied Stochastic Models in Business and Industry},
  Year                     = {2015},
  Note                     = {Special issue on actual impact and future perspectives on stochastic modelling in business and industry},
  Pages                    = {37--49},
  Volume                   = {31},

  Abstract                 = {The modern service economy is substantively different from the agricultural and manufacturing economies that preceded it. In particular, the cost of experimenting is dominated by opportunity cost rather than the cost of obtaining experimental units. The different economics require a new class of experiments, in which stochastic models play an important role. This article briefly summarizes mulit-armed bandit experiments, where the experimental design is modified as the experiment progresses to make the experiment as inexpensive as possible.},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.04}
}

@Article{j-Scott2010,
  Title                    = {{A modern Bayesian look at the multi-armed bandit}},
  Author                   = {Steven L. Scott},
  Journal                  = {Applied Stochastic Models in Business and Industry},
  Year                     = {2010},
  Number                   = {6},
  Pages                    = {639--658},
  Volume                   = {26},

  Doi                      = {10.1002/asmb.874},
  ISSN                     = {1526-4025},
  Keywords                 = {probability matching, exploration vs exploitation, sequential design, Bayesian adaptive design},
  Owner                    = {iurteaga},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Timestamp                = {2016.09.19}
}

@Article{j-Shahriari2016,
  Title                    = {{Taking the Human Out of the Loop: A Review of Bayesian Optimization}},
  Author                   = {Bobak Shahriari and Kevin Swersky and Ziyu Wang and Ryan P. Adams and Nando de Freitas},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {148-175},
  Volume                   = {104},

  Abstract                 = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  Doi                      = {10.1109/JPROC.2015.2494218},
  ISSN                     = {0018-9219},
  Keywords                 = {Bayes methods;Big Data;optimisation;storage allocation;Bayesian optimization;Big data application;human productivity;large-scale heterogeneous computing;massive complex software system;product quality;storage architecture;Bayes methods;Big data;Decision making;Design of experiments;Genomes;Linear programming;Optimization;Statistical analysis;Decision making;decision making;design of experiments;genomic medicine;optimization;response surface methodology;statistical learning},
  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InCollection{ic-Snelson2006,
  Title                    = {{Sparse Gaussian Processes using Pseudo-inputs}},
  Author                   = {Edward Snelson and Zoubin Ghahramani},
  Booktitle                = {Advances in Neural Information Processing Systems 18},
  Publisher                = {MIT Press},
  Year                     = {2006},
  Editor                   = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
  Pages                    = {1257--1264},

  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Srinivas2010,
  Title                    = {{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}},
  Author                   = {Niranjan Srinivas and Andreas Krause and Sham Kakade and Matthias Seeger},
  Booktitle                = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
  Year                     = {2010},

  Address                  = {USA},
  Pages                    = {1015--1022},
  Publisher                = {Omnipress},
  Series                   = {ICML'10},

  Acmid                    = {3104451},
  ISBN                     = {978-1-60558-907-7},
  Location                 = {Haifa, Israel},
  Numpages                 = {8},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@Article{j-Strehl2010,
  Title                    = {{Learning from Logged Implicit Exploration Data}},
  Author                   = {Alexander L. Strehl and John Langford and Sham M. Kakade},
  Journal                  = {CoRR},
  Year                     = {2010},
  Volume                   = {abs/1003.0120},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@InProceedings{ip-Strehl2010,
  Title                    = {{Learning from Logged Implicit Exploration Data}},
  Author                   = {Alexander L. Strehl and John Langford and Lihong Li and Sham Kakade},
  Booktitle                = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia,
 Canada.},
  Year                     = {2010},
  Pages                    = {2217--2225},

  Owner                    = {iurteaga},
  Timestamp                = {2016.10.19}
}

@Book{b-Sutton1998,
  Title                    = {{Reinforcement Learning: An Introduction}},
  Author                   = {Richard S. Sutton and Andrew G. Barto},
  Publisher                = {MIT Press: Cambridge, MA},
  Year                     = {1998},

  Owner                    = {iurteaga},
  Timestamp                = {2017.05.10}
}

@Article{j-Teh2010,
  author    = {Yee Whye Teh and Michael I. Jordan},
  title     = {{Hierarchical Bayesian nonparametric models with applications}},
  journal   = {Bayesian nonparametrics},
  year      = {2010},
  volume    = {1},
  pages     = {158--207},
  keywords  = {Hierarchical Bayesian; Hierarchical Nonparametric; Hierarchical Dirichlet Process; Pitman-Yor process; Indian Buffet; Beta process;},
  owner     = {iurteaga},
  publisher = {Camb. Ser. Stat. Probab. Math},
  timestamp = {2017.07.07},
}

@Article{j-Teh2006,
  Title                    = {{Hierarchical Dirichlet Processes}},
  Author                   = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2006},
  Number                   = {476},
  Pages                    = {1566-1581},
  Volume                   = {101},

  Abstract                 = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
  Doi                      = {10.1198/016214506000000302},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Teh2008,
  Title                    = {{Collapsed Variational Inference for HDP}},
  Author                   = {Yee Whye Teh and Kenichi Kurihara and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2008},
  Volume                   = {20},

  Keywords                 = {HDP, variational},
  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@InProceedings{ip-Teh2007,
  Title                    = {{A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation}},
  Author                   = {Yee Whye Teh and David Newman and Max Welling},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2007},
  Volume                   = {19},

  Owner                    = {iurteaga},
  Timestamp                = {2017.04.20}
}

@Article{j-Thompson1935,
  Title                    = {{On the Theory of Apportionment}},
  Author                   = {William R. Thompson},
  Journal                  = {American Journal of Mathematics},
  Year                     = {1935},
  Number                   = {2},
  Pages                    = {450-456},
  Volume                   = {57},

  ISSN                     = {00029327, 10806377},
  Owner                    = {iurteaga},
  Publisher                = {Johns Hopkins University Press},
  Timestamp                = {2016.09.20}
}

@Article{j-Thompson1933,
  Title                    = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
  Author                   = {William R. Thompson},
  Journal                  = {Biometrika},
  Year                     = {1933},
  Number                   = {3/4},
  Pages                    = {285-294},
  Volume                   = {25},

  ISSN                     = {00063444},
  Owner                    = {iurteaga},
  Publisher                = {[Oxford University Press, Biometrika Trust]},
  Timestamp                = {2016.09.19}
}

@InProceedings{ip-Titsias2009,
  Title                    = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
  Author                   = {Michalis Titsias},
  Booktitle                = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},

  Address                  = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  Editor                   = {David van Dyk and Max Welling},
  Month                    = {16--18 Apr},
  Pages                    = {567--574},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {5},

  Abstract                 = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  File                     = {titsias09a.pdf:http\://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf:PDF},
  Owner                    = {iurteaga},
  Timestamp                = {2018.05.16}
}

@InProceedings{ip-Urteaga2018,
  Title                    = {{Variational inference for the multi-armed contextual bandit}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Booktitle                = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  Year                     = {2018},

  Address                  = {Playa Blanca, Lanzarote, Canary Islands},
  Editor                   = {Amos Storkey and Fernando Perez-Cruz},
  Month                    = {09--11 Apr},
  Pages                    = {698--706},
  Publisher                = {PMLR},
  Series                   = {Proceedings of Machine Learning Research},
  Volume                   = {84},

  Abstract                 = {In many biomedical, science, and engineering problems, one must sequentially decide which action to take next so as to maximize rewards. One general class of algorithms for optimizing interactions with the world, while simultaneously learning how the world operates, is the multi-armed bandit setting and, in particular, the contextual bandit case. In this setting, for each executed action, one observes rewards that are dependent on a given context, available at each interaction with the world. The Thompson sampling algorithm has recently been shown to enjoy provable optimality properties for this set of problems, and to perform well in real-world settings. It facilitates generative and interpretable modeling of the problem at hand. Nevertheless, the design and complexity of the model limit its application, since one must both sample from the distributions modeled and calculate their expected rewards. We here show how these limitations can be overcome using variational inference to approximate complex models, applying to the reinforcement learning case advances developed for the inference case in the machine learning community over the past two decades. We consider contextual multi-armed bandit applications where the true reward distribution is unknown and complex, which we approximate with a mixture model whose parameters are inferred via variational inference. We show how the proposed variational Thompson sampling approach is accurate in approximating the true distribution, and attains reduced regrets even with complex reward distributions. The proposed algorithm is valuable for practical scenarios where restrictive modeling assumptions are undesirable.},
  File                     = {urteaga18a.pdf:http\://proceedings.mlr.press/v84/urteaga18a/urteaga18a.pdf:PDF},
  Owner                    = {iurteaga},
  Timestamp                = {2018.02.06}
}

@Article{j-Urteaga2018,
  Title                    = {{(Sequential) Importance Sampling Bandits}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Journal                  = {ArXiv e-prints},
  Year                     = {2018},

  Month                    = sep,

  Archiveprefix            = {arXiv},
  Eprint                   = {1709.03162},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.ML},
  Timestamp                = {2018.08.08}
}

@Article{j-Urteaga2018a,
  author        = {I{\~{n}}igo Urteaga and Chris Wiggins},
  journal       = {arXiv preprint arXiv:1808.02932},
  title         = {{Nonparametric Gaussian mixture models for the multi-armed contextual bandit}},
  year          = {2018},
  month         = sep,
  archiveprefix = {arXiv},
  eprint        = {1808.02932},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  owner         = {iurteaga},
  primaryclass  = {stat.ML},
  timestamp     = {2018.08.08},
  url           = {https://arxiv.org/abs/1808.02932},
}

@Article{j-Urteaga2017,
  Title                    = {{Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling}},
  Author                   = {I{\~{n}}igo Urteaga and Chris Wiggins},
  Journal                  = {ArXiv e-prints},
  Year                     = {2017},

  Month                    = sep,

  Archiveprefix            = {arXiv},
  Eprint                   = {1709.03162},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Computation, I.2.6},
  Owner                    = {iurteaga},
  Primaryclass             = {stat.ML},
  Timestamp                = {2018.08.08}
}

@Article{j-Wang2012,
  Title                    = {{Continuous time dynamic topic models}},
  Author                   = {Chong Wang and Dvid Blei and Dvaid Heckerman},
  Journal                  = {arXiv preprint arXiv:1206.3298},
  Year                     = {2012},

  Abstract                 = {In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Wang2009,
  Title                    = {{Markov Topic Models}},
  Author                   = {Chong Wang and Bo Thiesson and Chris Meek and David Blei},
  Booktitle                = {D. van Dyk and M. Welling (Eds.), Proceedings of The Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS) 2009, JMLR: W\&CP 5},
  Year                     = {2009},
  Month                    = {April},
  Pages                    = {583-590},
  Publisher                = {Journal of Machine Learning Research},

  Abstract                 = {We develop Markov topic models (MTMs), a novel family of generative probabilistic models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora. MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora. We derive an efficient estimation procedure with variational expectation-maximization. We study the performance of our models on a corpus of abstracts from six different computer science conferences. Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@InProceedings{ip-Wang2006,
  Title                    = {{Topics over Time: A non-Markov Continuous-time Model of Topical Trends}},
  Author                   = {Xuerui Wang and Andrew McCallum},
  Booktitle                = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {424--433},
  Publisher                = {ACM},
  Series                   = {KDD '06},

  Acmid                    = {1150450},
  Doi                      = {10.1145/1150402.1150450},
  ISBN                     = {1-59593-339-5},
  Keywords                 = {graphical models, temporal analysis, topic modeling},
  Location                 = {Philadelphia, PA, USA},
  Numpages                 = {10},
  Owner                    = {iurteaga},
  Timestamp                = {2016.11.15}
}

@Book{b-Doucet2001,
  Title                    = {{Sequential Monte Carlo Methods in Practice}},
  Editor                   = {Arnaud Doucet and Nando De Freitas and Neil Gordon},
  Publisher                = {Springer},
  Year                     = {2001},

  Abstract                 = {{I: Introduction. An introduction to sequential Monte Carlo methods / Arnaud Doucet, Nando de Freitas, and Deil Gordon -- II: Theoretical issues. Particle filters-a theoretical perspective / Dan Crisan -- Interacting particle filtering with discrete observations / Pierre Del Moral and Jean Jacod / III: Strategies for improving sequential Monte Carlo methods. Sequential Monte Carlo methods for optimal filtering / Christophe Andrieu, Arnaud Doucet, and Elena Punskaya -- Deterministic and stochastic particle filters in state-space models / Erik Bolviken and Geir Storvik -- RESAMPLE-MOVE filtering with cross-model jumps / Carlo Berzuini and Walter Gilks -- Improvement strategies for Monte Carlo particle filters / Simon Godsill and Tim Clapp -- Approximating and maximising the likelihood for a general state-space model / Markus Hurzeler and Hans R. Kunsch -- Monte Carlo smoothing and self-organising state-space model / Genshiro Kitagawa and Seisho Sato -- Combined parameter and state estimation in simulation-based filtering / Jane Liu and Mike West -- A theoretical framework for sequential importance sampling with reasampling / Jun S. Liu, Rong Chen, and Tanya Logvinenko -- Improving regularised particle filters / Christian Musso, Nadia Oudjane, and Francois Le Gland -- Auxiliary variable based particle filters / Michael K. Pitt and Neil Shephard -- Improved particle filters and smoothing / Photis Stavropoulos and D.M. Titterington -- IV: Applications. Posterior Cramer-Rao bounds for sequential estimation / Niclas Bergman -- Statistical models of visual shape and motion / Andrew Blake, Michael Isard, and John MacCormick -- Sequential Monte Carlo methods for neural networks / N de Freitas...[et al.] -- Sequential estimation of signals under model uncertainty / Petar M. Djuric -- Particle filters for mobile robot localization / Dieter Fox...[et al.] -- Self-organizing time series model / Tomoyuki Higuchi -- Sampling in factored dynamic systems / Daphne Koller and Uri Lerner -- In-situ ellipsometry solutions using sequential Monte Carlo / Alan D. Marrs -- Manoeuvring target tracking using a multiple-model bootstrap filter / Shaun McGinnity and George W. Irwin -- Rao-Blackwellised particle filtering for dynamic Bayesian networks / Kevin Murphy and Stuart Russell -- Particles and mixtures for tracking and guidance / David Salmond and Neil Gordon -- Monte Carlo techniques for automated target recognition / Anuj Srivastava...[et al.].}},
  Keywords                 = {Monte Carlo Methods, filtering, Particle filtering, Sequential Monte Carlo;},
  Owner                    = {iurteaga},
  Timestamp                = {2014-06-03}
}

@Article{j-Cherkassky2013,
  author        = {Michael Cherkassky and Luke Bornn},
  title         = {{Sequential Monte Carlo Bandits}},
  journal       = {ArXiv e-prints},
  year          = {2013},
  month         = oct,
  archiveprefix = {arXiv},
  eprint        = {1310.1404},
  keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Methodology},
  primaryclass  = {stat.ML},
}

@Article{j-Raj2017,
  author  = {Vishnu Raj and Sheetal Kalyani},
  journal = {arXiv preprint arXiv:1707.09727},
  title   = {Taming non-stationary bandits: A Bayesian approach},
  year    = {2017},
  eprint  = {1707.09727},
  url     = {https://arxiv.org/abs/1707.09727},
}

@InCollection{ic-Besbes2014,
  author    = {Omar Besbes and Yonatan Gur and Assaf Zeevi},
  title     = {Stochastic Multi-Armed-Bandit Problem with Non-stationary Rewards},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {199--207},
  url       = {http://papers.nips.cc/paper/5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards},
}

@InProceedings{ip-Mellor2013,
  author    = {Joseph Mellor and Jonathan Shapiro},
  booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  title     = {{Thompson Sampling in Switching Environments with Bayesian Online Change Detection}},
  year      = {2013},
  address   = {Scottsdale, Arizona, USA},
  editor    = {Carlos M. Carvalho and Pradeep Ravikumar},
  month     = {29 Apr--01 May},
  pages     = {442--450},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {31},
  abstract  = {Thompson Sampling has recently been shown to achieve the lower bound on regret in the Bernoulli Multi-Armed Bandit setting. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS).  We show empirical results in 4 artificial environments, and 2 derived from real world data: news click-through and foreign exchange data, comparing them to some other bandit algorithms. In real world data CTS is the most effective.},
  file      = {mellor13a.pdf:http\://proceedings.mlr.press/v31/mellor13a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v31/mellor13a.html},
}

@InProceedings{ip-Garivier2011,
  author    = {Aur{\'e}lien Garivier and Eric Moulines},
  title     = {On Upper-confidence Bound Policies for Switching Bandit Problems},
  booktitle = {Proceedings of the 22Nd International Conference on Algorithmic Learning Theory},
  year      = {2011},
  series    = {ALT'11},
  pages     = {174--188},
  address   = {Berlin, Heidelberg},
  publisher = {Springer-Verlag},
  acmid     = {2050365},
  isbn      = {978-3-642-24411-7},
  location  = {Espoo, Finland},
  numpages  = {15},
  url       = {http://dl.acm.org/citation.cfm?id=2050345.2050365},
}

@InCollection{ic-Dumitrascu2018,
  author    = {Bianca Dumitrascu and Karen Feng and Barbara Engelhardt},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  title     = {{PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits}},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {4629--4638},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/ce6c92303f38d297e263c7180f03d402-Abstract.html},
}

@InProceedings{ip-Lu2017,
  author    = {Xiuyuan Lu and Benjamin Van Roy},
  title     = {{Ensemble sampling}},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {3258--3266},
}

@Article{j-Osband2015,
  author  = {Ian Osband and Benjamin Van Roy},
  journal = {arXiv preprint arXiv:1507.00300},
  title   = {{Bootstrapped Thompson sampling and deep exploration}},
  year    = {2015},
  eprint  = {1507.00300},
  url     = {https://arxiv.org/abs/1507.00300},
}

@Article{j-Ghosal1999,
  author  = {Subhashis Ghosal and Jayanta K. Ghosh and R.V. Ramamoorthi},
  title   = {{Posterior consistency of Dirichlet mixtures in density estimation}},
  journal = {Annals of Statistics},
  year    = {1999},
  volume  = {27},
  number  = {1},
  pages   = {143--158},
  url     = {http://repository.ias.ac.in/22510/1/308.pdf},
}

@Article{j-Lijoi2004,
  author    = {Antonio Lijoi and Igor Pr{\"u}nster and Stephen G. Walker},
  title     = {{Extending Doob's consistency theorem to nonparametric densities}},
  journal   = {Bernoulli},
  year      = {2004},
  volume    = {10},
  number    = {4},
  pages     = {651--663},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability},
}

@Article{j-Miller2014,
  author  = {Jeffrey W. Miller and Matthew T. Harrison},
  title   = {{Inconsistency of Pitman-Yor Process Mixtures for the Number of Components}},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3333-3370},
  url     = {http://jmlr.org/papers/v15/miller14a.html},
}

@Article{j-Ghosal2007,
  author    = {Subhashis Ghosal and Aad van der Vaart},
  title     = {{Posterior convergence rates of Dirichlet mixtures at smooth densities}},
  journal   = {Ann. Statist.},
  year      = {2007},
  volume    = {35},
  number    = {2},
  pages     = {697--723},
  month     = {04},
  doi       = {10.1214/009053606000001271},
  fjournal  = {The Annals of Statistics},
  publisher = {The Institute of Mathematical Statistics},
}

@Article{j-Tokdar2006,
  author    = {Surya T. Tokdar},
  title     = {{Posterior consistency of Dirichlet location-scale mixture of normals in density estimation and regression}},
  journal   = {Sankhy{\=a}: The Indian Journal of Statistics},
  year      = {2006},
  pages     = {90--110},
  publisher = {JSTOR},
}

@Article{j-Pati2013,
  author    = {Debdeep Pati and David B. Dunson and Surya T. Tokdar},
  title     = {{Posterior consistency in conditional distribution estimation}},
  journal   = {Journal of multivariate analysis},
  year      = {2013},
  volume    = {116},
  pages     = {456--472},
  publisher = {Elsevier},
}

@Article{j-Bhattacharya2010,
  author    = {Abhishek Bhattacharya and David B. Dunson},
  title     = {{Nonparametric Bayesian density estimation on manifolds with applications to planar shapes}},
  journal   = {Biometrika},
  year      = {2010},
  volume    = {97},
  number    = {4},
  pages     = {851--865},
  publisher = {Oxford University Press},
}

@Article{j-Gottesman2019,
  author   = {Omer Gottesman and Fredrik Johansson and Matthieu Komorowski and Aldo Faisal and David Sontag and Finale Doshi-Velez and Leo Anthony Celi},
  title    = {{Guidelines for reinforcement learning in healthcare}},
  journal  = {Nature Medicine},
  year     = {2019},
  volume   = {25},
  pages    = {16--18},
  abstract = {In this Comment, we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe, risk-conscious manner.},
  doi      = {https://doi.org/10.1038/s41591-018-0310-5},
}

@Article{j-Ghosal2001,
  author    = {Subhashis Ghosal and Aad W. van der Vaart},
  title     = {{Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities}},
  journal   = {Ann. Statist.},
  year      = {2001},
  volume    = {29},
  number    = {5},
  pages     = {1233--1263},
  month     = {10},
  doi       = {10.1214/aos/1013203452},
  fjournal  = {The Annals of Statistics},
  publisher = {The Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aos/1013203452},
}

@Article{j-Ghosal2000,
  author    = {Subhashis Ghosal and Jayanta K. Ghosh and Aad W. van der Vaart},
  title     = {{Convergence rates of posterior distributions}},
  journal   = {Ann. Statist.},
  year      = {2000},
  volume    = {28},
  number    = {2},
  pages     = {500--531},
  month     = {04},
  doi       = {10.1214/aos/1016218228},
  fjournal  = {The Annals of Statistics},
  publisher = {The Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aos/1016218228},
}

@Book{b-Ibragimov1981,
  title     = {{Statistical Estimation: Asymptotic Theory}},
  publisher = {Springer},
  year      = {1981},
  author    = {I.A. Ibragimov and R.Z. Has'minskii},
  editor    = {A. V. Balakrishnan},
  doi       = {10.1007/978-1-4899-0027-2},
}

@Article{j-Battiston2018,
  author    = {Marco Battiston and Stefano Favaro and Yee Whye Teh},
  journal   = {Journal of the American Statistical Association},
  title     = {{Multi-Armed Bandit for Species Discovery: A Bayesian Nonparametric Approach}},
  year      = {2018},
  number    = {521},
  pages     = {455-466},
  volume    = {113},
  abstract  = {Let (P1,...,PJ) denote J populations of animals from distinct regions. A priori, it is unknown which species are present in each region and what are their corresponding frequencies. Species are shared among populations and each species can be present in more than one region with its frequency varying across populations. In this article, we consider the problem of sequentially sampling these populations to observe the greatest number of different species. We adopt a Bayesian nonparametric approach and endow (P1,...,PJ) with a hierarchical Pitman-Yor process prior. As a consequence of the hierarchical structure, the J unknown discrete probability measures share the same support, that of their common random base measure. Given this prior choice, we propose a sequential rule that, at every time step, given the information available up to that point, selects the population from which to collect the next observation. Rather than picking the population with the highest posterior estimate of producing a new value, the proposed rule includes a Thompson sampling step to better balance the exploration-exploitation trade-off. We also propose an extension of the algorithm to deal with incidence data, where multiple observations are collected in a time period. The performance of the proposed algorithms is assessed through a simulation study and compared to three other strategies. Finally, we compare these algorithms using a dataset of species of trees, collected from different plots in South America. Supplementary materials for this article are available online.},
  doi       = {10.1080/01621459.2016.1261711},
  eprint    = {https://doi.org/10.1080/01621459.2016.1261711},
  publisher = {Taylor \& Francis},
  url       = {https://doi.org/10.1080/01621459.2016.1261711},
}

@Book{b-Ghosal2017,
  title     = {{Fundamentals of nonparametric Bayesian inference}},
  publisher = {Cambridge University Press},
  year      = {2017},
  author    = {Subhashis Ghosal and Aad Van der Vaart},
  volume    = {44},
}

@InProceedings{ip-Kandasamy2018,
  author    = {Kirthevasan Kandasamy and Akshay Krishnamurthy and Jeff Schneider and Barnabas Poczos},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  title     = {{Parallelised Bayesian Optimisation via Thompson Sampling}},
  year      = {2018},
  address   = {Playa Blanca, Lanzarote, Canary Islands},
  editor    = {Amos Storkey and Fernando Perez-Cruz},
  month     = {09--11 Apr},
  pages     = {133--142},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {84},
  abstract  = {We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making $n$ evaluations distributed among $M$ workers is essentially equivalent to performing $n$ evaluations in sequence. Further, by modelling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronous parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in an application involving tuning hyper-parameters of a convolutional neural network. In addition to these, the proposed procedure is conceptually much simpler than existing work for parallel BO.},
  url       = {http://proceedings.mlr.press/v84/kandasamy18a.html},
}

@InCollection{ic-Bai2013,
  author    = {Aijun Bai and Feng Wu and Xiaoping Chen},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  title     = {{Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search}},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {1646--1654},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
}

@Article{j-Ferreira2018,
  author   = {Kris Johnson Ferreira and David Simchi-Levi and He Wang},
  journal  = {Operations Research},
  title    = {{Online Network Revenue Management Using Thompson Sampling}},
  year     = {2018},
  number   = {6},
  pages    = {1586-1602},
  volume   = {66},
  abstract = {We consider a price-based network revenue management problem in which a retailer aims to maximize revenue from multiple products with limited inventory over a finite selling season. As is common in practice, we assume the demand function contains unknown parameters that must be learned from sales data. In the presence of these unknown demand parameters, the retailer faces a trade-off commonly referred to as the ``exploration-exploitation trade-off.'' Toward the beginning of the selling season, the retailer may offer several different prices to try to learn demand at each price (``exploration'' objective). Over time, the retailer can use this knowledge to set a price that maximizes revenue throughout the remainder of the selling season (``exploitation'' objective). We propose a class of dynamic pricing algorithms that builds on the simple, yet powerful, machine learning technique known as ``Thompson sampling'' to address the challenge of balancing the exploration-exploitation trade-off under the presence of inventory constraints. Our algorithms have both strong theoretical performance guarantees and promising numerical performance results when compared with other algorithms developed for similar settings. Moreover, we show how our algorithms can be extended for use in general multiarmed bandit problems with resource constraints as well as in applications in other revenue management settings and beyond.},
  doi      = {10.1287/opre.2018.1755},
  eprint   = {https://doi.org/10.1287/opre.2018.1755},
  url      = {https://doi.org/10.1287/opre.2018.1755},
}

@Article{j-Schwartz2017,
  author   = {Eric M. Schwartz and Eric T. Bradlow and Peter S. Fader},
  journal  = {Marketing Science},
  title    = {{Customer Acquisition via Display Advertising Using Multi-Armed Bandit Experiments}},
  year     = {2017},
  number   = {4},
  pages    = {500--522},
  volume   = {36},
  abstract = {Firms using online advertising regularly run experiments with multiple versions of their ads since they are uncertain about which ones are most effective. During a campaign, firms try to adapt to intermediate results of their tests, optimizing what they earn while learning about their ads. Yet how should they decide what percentage of impressions to allocate to each ad? This paper answers that question, resolving the well-known ``learn-and-earn'' trade-off using multi-armed bandit (MAB) methods. The online advertiser's MAB problem, however, contains particular challenges, such as a hierarchical structure (ads within a website), attributes of actions (creative elements of an ad), and batched decisions (millions of impressions at a time), that are not fully accommodated by existing MAB methods. Our approach captures how the impact of observable ad attributes on ad effectiveness differs by website in unobserved ways, and our policy generates allocations of impressions that can be used in practice. We implemented this policy in a live field experiment delivering over 750 million ad impressions in an online display campaign with a large retail bank. Over the course of two months, our policy achieved an 8\% improvement in the customer acquisition rate, relative to a control policy, without any additional costs to the bank. Beyond the actual experiment, we performed counterfactual simulations to evaluate a range of alternative model specifications and allocation rules in MAB policies. Finally, we show that customer acquisition would decrease by about 10\% if the firm were to optimize click-through rates instead of conversion directly, a finding that has implications for understanding the marketing funnel.},
  doi      = {10.1287/mksc.2016.1023},
  url      = {https://doi.org/10.1287/mksc.2016.1023},
}

@InProceedings{ip-Agarwal2013,
  author    = {Deepak Agarwal},
  title     = {{Computational Advertising: The Linkedin Way}},
  booktitle = {Proceedings of the 22Nd ACM International Conference on Information \& Knowledge Management},
  year      = {2013},
  series    = {CIKM '13},
  pages     = {1585--1586},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2514690},
  doi       = {10.1145/2505515.2514690},
  isbn      = {978-1-4503-2263-8},
  keywords  = {computational advertising, machine learning, social networks},
  location  = {San Francisco, California, USA},
  numpages  = {2},
  url       = {http://doi.acm.org/10.1145/2505515.2514690},
}

@InProceedings{ip-Hill2017,
  author       = {Daniel N Hill and Houssam Nassif and Yi Liu and Anand Iyer and SVN Vishwanathan},
  title        = {{An efficient bandit algorithm for realtime multivariate optimization}},
  booktitle    = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year         = {2017},
  pages        = {1813--1821},
  organization = {ACM},
}

@Misc{Netflix2017,
  author       = {Netflix},
  title        = {{Artwork Personalization at Netflix}},
  howpublished = {{\href{https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76}{medium.com}}},
  month        = {December},
  year         = {2017},
}

@InProceedings{ip-Garivier2011a,
  author    = {Aur\'{e}lien Garivier and Olivier Capp\'{e}},
  title     = {{The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond}},
  booktitle = {Proceedings of the 24th Annual Conference on Learning Theory},
  year      = {2011},
  editor    = {Sham M. Kakade and Ulrike von Luxburg},
  volume    = {19},
  series    = {Proceedings of Machine Learning Research},
  pages     = {359--376},
  address   = {Budapest, Hungary},
  month     = {09--11 Jun},
  publisher = {PMLR},
  abstract  = {This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free  index policy for stochastic bandit problems.  We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm  satisfies a uniformly better regret bound than UCB and its variants; second, in the special case of  Bernoulli rewards, it reaches the lower bound of Lai and Robbins.  Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for  specific classes of (possibly unbounded) rewards, including those generated from exponential  families of distributions.  A large-scale numerical study comparing KL-UCB with its main competitors (UCB, MOSS,  UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better  than the basic UCB policy.  Our regret bounds rely on deviations results of independent interest which are stated and proved  in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB  algorithm.},
  file      = {garivier11a.pdf:http\://proceedings.mlr.press/v19/garivier11a/garivier11a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v19/garivier11a.html},
}

@InProceedings{ip-Gopalan2014,
  author    = {Aditya Gopalan and Shie Mannor and Yishay Mansour},
  title     = {{Thompson Sampling for Complex Online Problems}},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  year      = {2014},
  editor    = {Eric P. Xing and Tony Jebara},
  volume    = {32},
  number    = {1},
  series    = {Proceedings of Machine Learning Research},
  pages     = {100--108},
  address   = {Bejing, China},
  month     = {22--24 Jun},
  publisher = {PMLR},
  abstract  = {We consider stochastic multi-armed bandit problems with complex actions over a set of basic arms, where the decision maker plays a complex action rather than a basic arm in each round. The reward of the complex action is some function of the basic arms' rewards, and the feedback observed may not necessarily be the reward per-arm. For instance, when the complex actions are subsets of the arms, we may only observe the maximum reward over the chosen subset. Thus, feedback across complex actions may be coupled due to the nature of the reward function. We prove a frequentist regret bound for Thompson sampling in a very general setting involving parameter, action and observation spaces and a likelihood function over them. The bound holds for discretely-supported priors over the parameter space and without additional structural properties such as closed-form posteriors, conjugate prior structure or independence across arms. The regret bound scales logarithmically with time but, more importantly, with an improved constant that non-trivially captures the coupling across complex actions due to the structure of the rewards. As applications, we derive improved regret bounds for classes of complex bandit problems involving selecting subsets of arms, including the first nontrivial regret bounds for nonlinear MAX reward feedback from subsets. Using particle filters for computing posterior distributions which lack an explicit closed-form, we present numerical results for the performance of Thompson sampling for subset-selection and job scheduling problems.},
  file      = {gopalan14.pdf:http\://proceedings.mlr.press/v32/gopalan14.pdf:PDF},
  url       = {http://proceedings.mlr.press/v32/gopalan14.html},
}

@Article{j-Eckles2019,
  author   = {Dean Eckles and Maurits Kaptein},
  journal  = {SAGE Open},
  title    = {{Bootstrap Thompson Sampling and Sequential Decision Problems in the Behavioral Sciences}},
  year     = {2019},
  number   = {2},
  pages    = {2158244019851675},
  volume   = {9},
  abstract = {Behavioral scientists are increasingly able to conduct randomized experiments in settings that enable rapidly updating probabilities of assignment to treatments (i.e., arms). Thus, many behavioral science experiments can be usefully formulated as sequential decision problems. This article reviews versions of the multiarmed bandit problem with an emphasis on behavioral science applications. One popular method for such problems is Thompson sampling, which is appealing for randomizing assignment and being asymptoticly consistent in selecting the best arm. Here, we show the utility of bootstrap Thompson sampling (BTS), which replaces the posterior distribution with the bootstrap distribution. This often has computational and practical advantages. We illustrate its robustness to model misspecification, which is a common concern in behavioral science applications. We show how BTS can be readily adapted to be robust to dependent data, such as repeated observations of the same units, which is common in behavioral science applications. We use simulations to illustrate parametric Thompson sampling and BTS for Bernoulli bandits, factorial Gaussian bandits, and bandits with repeated observations of the same units.},
  doi      = {10.1177/2158244019851675},
  eprint   = {https://doi.org/10.1177/2158244019851675},
  url      = {https://doi.org/10.1177/2158244019851675},
}

@Book{b-Box1976,
  author    = {G.E.P. Box and G.M. Jenkins},
  publisher = {Holden-Day},
  title     = {{Time Series Analysis: Forecasting and Control}},
  year      = {1976},
  isbn      = {9780816211043},
  series    = {Holden-Day series in time series analysis and digital processing},
  lccn      = {76008713},
  owner     = {iurteaga},
  timestamp = {2014-12-27},
}

@Book{b-Brockwell1991,
  author    = {Peter J. Brockwell and Richard A. Davis},
  publisher = {Springer},
  title     = {{Time Series: Theory and Methods}},
  year      = {1991},
  edition   = {2nd},
  isbn      = {1441903194,9781441903198},
  series    = {Springer Series in Statistics},
  keywords  = {Time Series, ARMA},
  owner     = {iurteaga},
  timestamp = {2013-11-14},
}

@Book{b-Durbin2001,
  author    = {James Durbin and Siem Jan Koopman},
  publisher = {Oxford University Press},
  title     = {{Time Series Analysis by State-Space Methods}},
  year      = {2001},
  series    = {Oxford Statistical Science Series},
  abstract  = {This excellent text provides a comprehensive treatment of the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbence terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. The book provides an excellent source for the development of practical courses on time series analysis.},
  keywords  = {Linear Gaussian; State-Space Model; ARMA models; ARIMA models; Non-linear; non-gaussian ;},
  owner     = {iurteaga},
  timestamp = {2013-07-26},
}

@Book{b-Durbin2012,
  author    = {James Durbin and Siem Jan Koopman},
  publisher = {Oxford University Press},
  title     = {{Time Series Analysis by State-Space Methods: Second Edition}},
  year      = {2012},
  edition   = {2},
  isbn      = {9780199641178},
  series    = {Oxford Statistical Science Series},
  abstract  = {This excellent text provides a comprehensive treatment of the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbence terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. The book provides an excellent source for the development of practical courses on time series analysis.},
  keywords  = {Linear Gaussian; State-Space Model; ARMA models; ARIMA models; Non-linear; non-gaussian ;},
  lccn      = {2011945385},
  owner     = {iurteaga},
  timestamp = {2014-12-26},
}

@Book{b-Whittle1951,
  title     = {{Hypothesis Testing in Time Series Analysis}},
  publisher = {Almquist and Wicksell},
  year      = {1951},
  author    = {Peter Whittle},
  owner     = {iurteaga},
  timestamp = {2015-08-11},
}

@Book{b-Shumway2010,
  author    = {Robert H. Shumway and David S. Stoffer},
  publisher = {Springer},
  title     = {{Time Series Analysis and Its Applications: With R Examples (Springer Texts in Statistics)}},
  year      = {2010},
  edition   = {3rd},
  isbn      = {144197864X,9781441978646},
  abstract  = {Time Series Analysis and Its Applications presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using nontrivial data illustrate solutions to problems such as discovering natural and anthropogenic climate change, evaluating pain perception experiments using functional magnetic resonance imaging, and monitoring a nuclear test ban treaty. The book is designed to be useful as a text for graduate level students in the physical, biological and social sciences and as a graduate level text in statistics. Some parts may also serve as an undergraduate introductory course. Theory and methodology are separated to allow presentations on different levels. In addition to coverage of classical methods of time series regression,&nbsp;ARIMA models, spectral analysis and state-space models, the text includes modern developments including categorical time series analysis, multivariate spectral methods, long memory series, nonlinear models, resampling techniques, GARCH models, stochastic volatility, wavelets and Monte Carlo Markov chain integration methods.&nbsp;The third edition includes a new section on testing for unit roots and the material on state-space modeling, ARMAX models, and regression with autocorrelated errors have been expanded. Also new to this edition is the enhanced use of the freeware statistical package R.&nbsp;In particular, R code is now included in the text for nearly all of the numerical examples.&nbsp;Data sets and additional R scripts are now provided in one file that may be downloaded via the World Wide Web.&nbsp;This R supplement is a small compressed file that can be loaded easily into R making all the data sets and scripts available to the user with one simple command.&nbsp;The website for the text includes the code used in each example so that the reader may simply copy-and-paste code directly into R.&nbsp;Appendix R, which is new to this edition, provides a reference for the data sets and our R scripts that are used throughout the text. In addition, Appendix R includes a tutorial on basic R commands as well as an R time series tutorial.},
  keywords  = {Time series; ARIMA models, State-Space Models;},
  owner     = {iurteaga},
  timestamp = {2013-07-26},
}

@Article{j-Urteaga2017b,
  author    = {I{\~n}igo Urteaga and M\'{o}nica F. Bugallo and Petar M. Djuri\'{c}},
  title     = {{Sequential Monte Carlo for inference of latent ARMA time-series with innovations correlated in time}},
  journal   = {EURASIP Journal on Advances in Signal Processing},
  year      = {2017},
  volume    = {2017},
  number    = {1},
  month     = {Dec},
  doi       = {10.1186/s13634-017-0518-4},
  owner     = {iurteaga},
  timestamp = {2016-10-25},
  url       = {https://doi.org/10.1186/s13634-017-0518-4},
}

@Article{j-Urteaga2016,
  author    = {I{\~n}igo Urteaga and Petar M. Djuri\'{c}},
  title     = {{Sequential Estimation of Hidden {ARMA} Processes by Particle Filtering - {P}art {I}}},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2016},
  volume    = {PP},
  number    = {99},
  pages     = {1-1},
  issn      = {1053-587X},
  abstract  = {This paper is Part I of a series of two papers where we address sequential estimation of wide-sense stationary autoregressive moving average (ARMA) state processes by particle filtering. In Part I, we present estimation methods for ARMA processes of known model order, where the parameters are first known and then unknown. The driving noise of the ARMA process is Gaussian with unknown variance. We derive the transition density of the ARMA state for settings that correspond to different assumptions of a priori knowledge. Instead of estimating all the unknown parameters of the model, we treat them by Rao-Blackwellization. We propose a particle filtering method, with appropriate variations according to available information, for sequential estimation of the unknown state as it evolves with time. We demonstrate the performance of the proposed methods by extensive computer simulations.},
  doi       = {10.1109/TSP.2016.2598309},
  keywords  = {Atmospheric measurements;Autoregressive processes;Computational modeling;Estimation;Geophysical measurements;Particle measurements;Signal processing;ARMA processes;Rao-Blackwellization;known model order;nonlinear models;particle filtering},
  owner     = {iurteaga},
  timestamp = {2015-10-05},
}

@Article{j-Urteaga2016a,
  author    = {I{\~n}igo Urteaga and Petar M. Djuri\'{c}},
  title     = {{Sequential Estimation of Hidden {ARMA} Processes by Particle Filtering - {P}art {II}}},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2016},
  volume    = {PP},
  number    = {99},
  pages     = {1-1},
  issn      = {1053-587X},
  abstract  = {This is Part II of a series of two papers where we address sequential estimation of wide-sense stationary autoregressive moving average (ARMA) state processes by particle filtering. In Part I, we considered a state-space model where the state was an ARMA process of known order and where the parameters of the process could be known or unknown. In this paper, we extend our work from Part I by considering the same type of models, with the added complexity that the ARMA processes are now of unknown order. Instead of working on a scheme that first tracks the state by operating with different assumed models, and then selects the best model by using a predefined criterion, we present a method that directly estimates the state without the need of knowing the model order.We derive the transition density of the state for unknown ARMA model order, and propose a particle filter based on that density and the empirical Bayesian methodology. We demonstrate the performance of the proposed method with computer simulations and compare it with the methods from Part I.},
  doi       = {10.1109/TSP.2016.2598324},
  keywords  = {Autoregressive processes;Bayes methods;Computational modeling;Covariance matrices;Estimation;Signal processing;State-space methods;ARMA processes;Rao-Blackwellization;empirical Bayes;nonlinear models;particle filtering;unknown model order},
  owner     = {iurteaga},
  timestamp = {2015-10-05},
}

@Article{j-Kantas2015,
  author    = {Nikolas Kantas and Arnaud Doucet and Sumeetpal S Singh and Jan Maciejowski and Nicolas Chopin},
  title     = {{On particle methods for parameter estimation in state-space models}},
  journal   = {Statistical science},
  year      = {2015},
  volume    = {30},
  number    = {3},
  pages     = {328--351},
  publisher = {Institute of Mathematical Statistics},
}

@InCollection{ic-Kawale2015,
  author    = {Jaya Kawale and Hung H Bui and Branislav Kveton and Long Tran-Thanh and Sanjay Chawla},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  title     = {{Efficient Thompson Sampling for Online Matrix-Factorization Recommendation}},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {1297--1305},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/846c260d715e5b854ffad5f70a516c88-Abstract.html},
}

@InProceedings{ip-Merwe2001,
  author    = {Rudolph Van Der Merwe and Arnaud Doucet and Nando De Freitas and Eric A Wan},
  title     = {{The unscented particle filter}},
  booktitle = {Advances in neural information processing systems},
  year      = {2001},
  pages     = {584--590},
}

@Article{j-Andrieu2010,
  author    = {Christophe Andrieu and Arnaud Doucet and Roman Holenstein},
  title     = {{Particle markov chain monte carlo methods}},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year      = {2010},
  volume    = {72},
  number    = {3},
  pages     = {269--342},
  publisher = {Wiley Online Library},
}

@InCollection{ic-Filippi2010,
  author    = {Sarah Filippi and Olivier Cappe and Aur\'{e}lien Garivier and Csaba Szepesv\'{a}ri},
  title     = {{Parametric Bandits: The Generalized Linear Case}},
  booktitle = {Advances in Neural Information Processing Systems 23},
  publisher = {Curran Associates, Inc.},
  year      = {2010},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  pages     = {586--594},
  url       = {http://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case.pdf},
}

@Article{j-Garivier2019,
  author   = {Aur\'{e}lien Garivier and Pierre M\'{e}nard and Gilles Stoltz},
  journal  = {Mathematics of Operations Research},
  title    = {{Explore First, Exploit Next: The True Shape of Regret in Bandit Problems}},
  year     = {2019},
  number   = {2},
  pages    = {377-399},
  volume   = {44},
  abstract = {We revisit lower bounds on the regret in the case of multi-armed bandit problems. We obtain nonasymptotic, distribution-dependent bounds and provide simple proofs based only on well-known properties of Kullback-Leibler divergences. These bounds show in particular that in the initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the information-theoretic arguments used and they involve no unnecessary complications.},
  doi      = {10.1287/moor.2017.0928},
  url      = {https://doi.org/10.1287/moor.2017.0928},
}

@Article{j-Rindtorff2019,
  author  = {Niklas T. Rindtorff and MingYu Lu and Nisarg A. Patel and Huahua Zheng and Alexander D'Amour},
  title   = {A Biologically Plausible Benchmark for Contextual Bandit Algorithms in Precision Oncology Using in vitro Data},
  journal = {arXiv preprint arXiv:1911.04389},
  year    = {2019},
}

@InProceedings{ip-Hernandez-Lobato2016,
  author    = {Jose Hernandez-Lobato and Yingzhen Li and Mark Rowland and Thang Bui and Daniel Hernandez-Lobato and Richard Turner},
  title     = {Black-Box Alpha Divergence Minimization},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning,},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1511--1520},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  file      = {hernandez-lobatob16.pdf:http\://proceedings.mlr.press/v48/hernandez-lobatob16.pdf:PDF},
  url       = {http://proceedings.mlr.press/v48/hernandez-lobatob16.html},
}

@InProceedings{ip-Plappert2018,
  author    = {Matthias Plappert and Rein Houthooft and Prafulla Dhariwal and Szymon Sidor and Richard Y Chen and Xi Chen and Tamim Asfour and Pieter Abbeel and Marcin Andrychowicz},
  title     = {{Parameter Space Noise for Exploration}},
  booktitle = {International Conference on Learning Representations},
  year      = {2018},
}

@InCollection{ic-Bubeck2013,
  author    = {Sebastien Bubeck and Che-Yu Liu},
  title     = {{Prior-free and prior-dependent regret bounds for Thompson Sampling}},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {638--646},
  url       = {http://papers.nips.cc/paper/5108-prior-free-and-prior-dependent-regret-bounds-for-thompson-sampling.pdf},
}

@InProceedings{ip-Agrawal2013a,
  author    = {Shipra Agrawal and Navin Goyal},
  title     = {{Thompson Sampling for Contextual Bandits with Linear Payoffs}},
  booktitle = {{International Conference on Machine Learning}},
  year      = {2013},
  pages     = {127--135},
}

@InProceedings{ip-Agrawal2013,
  author    = {Shipra Agrawal and Navin Goyal},
  title     = {{Further Optimal Regret Bounds for Thompson Sampling}},
  booktitle = {{Artificial Intelligence and Statistics}},
  year      = {2013},
  pages     = {99--107},
}

@InProceedings{ip-Agrawal2012,
  author    = {Shipra Agrawal and Navin Goyal},
  title     = {{Analysis of Thompson Sampling for the multi-armed bandit problem}},
  booktitle = {{Conference on Learning Theory}},
  year      = {2012},
  pages     = {39--1},
}

@InProceedings{ip-Lipton2018,
  author    = {Zachary C. Lipton and Xiujun Li and Jianfeng Gao and Lihong Li and Faisal Ahmed and Li Deng},
  title     = {{BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems}},
  booktitle = {AAAI},
  year      = {2018},
}

@Book{b-Lattimore2020,
  title     = {Bandit algorithms},
  publisher = {Cambridge University Press},
  year      = {2020},
  author    = {Tor Lattimore and Csaba Szepesv{\'a}ri},
}

@InProceedings{ip-Li2010,
  author    = {Lihong Li and Wei Chu and John Langford and Robert E. Schapire},
  title     = {{A Contextual-Bandit Approach to Personalized News Article Recommendation}},
  booktitle = {Proceedings of the 19th international conference on World wide web},
  year      = {2010},
  volume    = {abs/1003.0146},
  pages     = {661--670},
  owner     = {iurteaga},
  timestamp = {2017.04.04},
}

@InProceedings{ip-Kveton2019,
  author    = {Branislav Kveton and Csaba Szepesvari and Sharan Vaswani and Zheng Wen and Tor Lattimore and Mohammad Ghavamzadeh},
  title     = {{Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3601--3610},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  abstract  = {We propose a bandit algorithm that explores by randomizing its history of rewards. Specifically, it pulls the arm with the highest mean reward in a non-parametric bootstrap sample of its history with pseudo rewards. We design the pseudo rewards such that the bootstrap mean is optimistic with a sufficiently high probability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $\Delta$ is the difference in the expected rewards of the optimal and the best suboptimal arms, and $K$ is the number of arms. The main advantage of our exploration design is that it easily generalizes to structured problems. To show this, we propose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and observe that it performs well.},
  file      = {kveton19a.pdf:http\://proceedings.mlr.press/v97/kveton19a/kveton19a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v97/kveton19a.html},
}

@InProceedings{ip-Li2020,
  author       = {Chang Li and Branislav Kveton and Tor Lattimore and Ilya Markov and Maarten de Rijke and Csaba Szepesv{\'a}ri and Masrour Zoghi},
  title        = {{BubbleRank: Safe online learning to re-rank via implicit click feedback}},
  booktitle    = {Uncertainty in Artificial Intelligence},
  year         = {2020},
  pages        = {196--206},
  organization = {PMLR},
}

@InProceedings{ip-Kveton2019a,
  author    = {Branislav Kveton and Csaba Szepesvari and Mohammad Ghavamzadeh and Craig Boutilier},
  title     = {{Perturbed-History Exploration in Stochastic Multi-Armed Bandits}},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
  year      = {2019},
  pages     = {2786--2793},
  month     = {7},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  doi       = {10.24963/ijcai.2019/386},
}

@Book{b-Mueller2015,
  title     = {Bayesian nonparametric data analysis},
  publisher = {Springer},
  year      = {2015},
  author    = {Peter M{\"u}ller and Fernando Andr{\'e}s Quintana and Alejandro Jara and Tim Hanson},
  doi       = {10.1007/978-3-319-18968-0},
}

@Article{j-Ghosal2010,
  author    = {Subhashis Ghosal},
  title     = {{The Dirichlet process, related priors and posterior asymptotics}},
  journal   = {Bayesian nonparametrics},
  year      = {2010},
  volume    = {28},
  pages     = {35},
  publisher = {Cambridge University Press Cambridge},
}

@Article{j-Escobar1995,
  author    = {Michael D Escobar and Mike West},
  journal   = {Journal of the American statistical association},
  title     = {Bayesian density estimation and inference using mixtures},
  year      = {1995},
  number    = {430},
  pages     = {577--588},
  volume    = {90},
  publisher = {Taylor \& Francis},
}

@Article{j-Mauldin1992,
  author    = {R Daniel Mauldin and William D Sudderth and Stanley C Williams},
  title     = {Polya trees and random distributions},
  journal   = {The Annals of Statistics},
  year      = {1992},
  pages     = {1203--1221},
  publisher = {JSTOR},
}

@Article{j-pitmanyor1997,
  author    = {Jim Pitman and Marc Yor},
  journal   = {The Annals of Probability},
  title     = {{The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator}},
  year      = {1997},
  number    = {2},
  pages     = {855 -- 900},
  volume    = {25},
  abstract  = {The two-parameter Poisson-Dirichlet distribution, denoted $\mathsf{PD}(\alpha, \theta)$ is a probability distribution on the set of decreasing positive sequences with sum 1. The usual Poisson-Dirichlet distribution with a single parameter $\theta$, introduced by Kingman, is $\mathsf{PD}(0, \theta)$. Known properties of $\mathsf{PD}(0, \theta)$, including the Markov chain description due to Vershik, Shmidt and Ignatov, are generalized to the two-parameter case. The size-biased random permutation of $\mathsf{PD}(\alpha, \theta)$ is a simple residual allocation model proposed by Engen in the context of species diversity, and rediscovered by Perman and the authors in the study of excursions of Brownian motion and Bessel processes. For $0 < \alpha < 1, \mathsf{PD}(\alpha, 0)$ is the asymptotic distribution of ranked lengths of excursions of a Markov chain away from a state whose recurrence time distribution is in the domain of attraction of a stable law of index $\alpha$. Formulae in this case trace back to work of Darling, Lamperti and Wendel in the 1950s and 1960s. The distribution of ranked lengths of excursions of a one-dimensional Brownian motion is $\mathsf{PD}(1/2, 0)$, and the corresponding distribution for a Brownian bredge is $\mathsf{PD}(1/2, 1/2)$. The $\mathsf{PD}(\alpha, 0)$ and $\mathsf{PD}(\alpha, \alpha)$ distributions admit a similar interpretation in terms of the ranked lengths of excursions of a semistable Markov process whose zero set is the range of a stable subordinator of index $\alpha$.},
  doi       = {10.1214/aop/1024404422},
  keywords  = {Local time, Poisson point process, ranked lengths of excursions, semistable Markov process, Zero set},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aop/1024404422},
}

@Article{j-Ishwaran2001,
  author    = {Hemant Ishwaran and Lancelot F. James},
  journal   = {Journal of the American Statistical Association},
  title     = {{Gibbs Sampling Methods for Stick-Breaking Priors}},
  year      = {2001},
  number    = {453},
  pages     = {161--173},
  volume    = {96},
  doi       = {10.1198/016214501750332758},
  eprint    = {https://doi.org/10.1198/016214501750332758},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.1198/016214501750332758},
}

@Article{j-Buntine2010,
  author  = {Buntine, Wray and Hutter, Marcus},
  journal = {arXiv preprint arXiv:1007.0296},
  title   = {{A Bayesian view of the Poisson-Dirichlet process}},
  year    = {2010},
}

@Article{j-Jang2010,
  author    = {Gun Ho Jang and Jaeyong Lee and Sangyeol Lee},
  journal   = {Statistica Sinica},
  title     = {{Posterior consistency of species sampling priors}},
  year      = {2010},
  pages     = {581--593},
  publisher = {JSTOR},
}

@Article{j-Scricciolo2014,
  author    = {Catia Scricciolo},
  journal   = {Bayesian Analysis},
  title     = {{Adaptive Bayesian density estimation in Lp-metrics with Pitman-Yor or normalized inverse-Gaussian process kernel mixtures}},
  year      = {2014},
  number    = {2},
  pages     = {475--520},
  volume    = {9},
  publisher = {International Society for Bayesian Analysis},
}

@Article{j-Fearnhead2004,
  author    = {Paul Fearnhead},
  journal   = {{Statistics and Computing}},
  title     = {{Particle filters for mixture models with an unknown number of components}},
  year      = {2004},
  number    = {1},
  pages     = {11--21},
  volume    = {14},
  publisher = {Springer},
}

@Article{j-Slivkins2019,
  author  = {Aleksandrs Slivkins},
  journal = {Foundations and Trends in Machine Learning},
  title   = {{Introduction to Multi-Armed Bandits}},
  year    = {2019},
  issn    = {1935-8237},
  number  = {1-2},
  pages   = {1-286},
  volume  = {12},
  doi     = {10.1561/2200000068},
  url     = {http://dx.doi.org/10.1561/2200000068},
}

@Article{j-Villar2015,
  author    = {Sofia Villar and Jack Bowden and James Wason},
  journal   = {Statistical Science},
  title     = {{Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges}},
  year      = {2015},
  number    = {2},
  pages     = {199 -- 215},
  volume    = {30},
  abstract  = {Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice.},
  doi       = {10.1214/14-STS504},
  keywords  = {Gittins index, multi-armed bandit, patient allocation, response adaptive procedures, Whittle index},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/14-STS504},
}

@Article{j-Murphy2005,
  author    = {Susan A. Murphy},
  journal   = {Statistics in medicine},
  title     = {{An experimental design for the development of adaptive treatment strategies}},
  year      = {2005},
  number    = {10},
  pages     = {1455--1481},
  volume    = {24},
  publisher = {Wiley Online Library},
}

@Article{j-Murphy2003,
  author    = {Susan A. Murphy},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title     = {{Optimal dynamic treatment regimes}},
  year      = {2003},
  number    = {2},
  pages     = {331--355},
  volume    = {65},
  publisher = {Wiley Online Library},
}

@Article{j-Gottesman2019,
  author   = {Omer Gottesman and Fredrik Johansson and Matthieu Komorowski and Aldo Faisal and David Sontag and Finale Doshi-Velez and Leo Anthony Celi},
  title    = {{Guidelines for reinforcement learning in healthcare}},
  journal  = {Nature Medicine},
  year     = {2019},
  volume   = {25},
  pages    = {16--18},
  abstract = {In this Comment, we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe, risk-conscious manner.},
  doi      = {https://doi.org/10.1038/s41591-018-0310-5},
}

@Article{j-Nahum-Shani2017,
  author    = {Inbal Nahum-Shani and Shawna N Smith and Bonnie J Spring and Linda M Collins and Katie Witkiewitz and Ambuj Tewari and Susan A Murphy},
  title     = {{Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support}},
  journal   = {Annals of Behavioral Medicine},
  year      = {2017},
  volume    = {52},
  number    = {6},
  pages     = {446--462},
  publisher = {Oxford University Press US},
}

@Article{j-Klasnja2019,
  author    = {Predrag Klasnja and Shawna Smith and Nicholas J Seewald and Andy Lee and Kelly Hall and Brook Luers and Eric B Hekler and Susan A Murphy},
  title     = {{Efficacy of contextually tailored suggestions for physical activity: A micro-randomized optimization trial of HeartSteps}},
  journal   = {Annals of Behavioral Medicine},
  year      = {2019},
  volume    = {53},
  number    = {6},
  pages     = {573--582},
  publisher = {Oxford University Press US},
}

@Article{j-Camerlenghi2020,
  author    = {Federico Camerlenghi and Bianca Dumitrascu and Federico Ferrari and Barbara E. Engelhardt and Stefano Favaro},
  journal   = {The Annals of Applied Statistics},
  title     = {{Nonparametric Bayesian multi-armed bandits for single-cell experiment design}},
  year      = {2020},
  number    = {4},
  pages     = {2003 -- 2019},
  volume    = {14},
  abstract  = {The problem of maximizing cell type discovery under budget constraints is a fundamental challenge for the collection and analysis of single-cell RNA-sequencing (scRNA-seq) data. In this paper we introduce a simple, computationally efficient and scalable Bayesian nonparametric sequential approach to optimize the budget allocation when designing a large-scale experiment for the collection of scRNA-seq data for the purpose of, but not limited to, creating cell atlases. Our approach relies on the following tools: (i) a hierarchical Pitman–Yor prior that recapitulates biological assumptions regarding cellular differentiation, and (ii) a Thompson sampling multiarmed bandit strategy that balances exploitation and exploration to prioritize experiments across a sequence of trials. Posterior inference is performed by using a sequential Monte Carlo approach which allows us to fully exploit the sequential nature of our species sampling problem. We empirically show that our approach outperforms state-of-the-art methods and achieves near-Oracle performance on simulated and scRNA-seq data alike. HPY-TS code is available at https://github.com/fedfer/HPYsinglecell.},
  doi       = {10.1214/20-AOAS1370},
  keywords  = {Cell type discovery, experimental sampling design, hierarchical Pitman–Yor model, multiarmed bandits, scRNA-seq, sequential Monte Carlo, Thompson sampling},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/20-AOAS1370},
}

@Book{b-Hjort2010,
  author    = {Nils Lid Hjort and Chris Holmes and Peter M{\"u}ller and Stephen G Walker},
  publisher = {Cambridge University Press},
  title     = {{Bayesian nonparametrics}},
  year      = {2010},
  volume    = {28},
}

@Article{j-Antoniak1974,
  author    = {Charles E. Antoniak},
  journal   = {The annals of statistics},
  title     = {{Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems}},
  year      = {1974},
  pages     = {1152--1174},
  publisher = {JSTOR},
}

@Article{j-Ferguson1973,
  author    = {Thomas S. Ferguson},
  journal   = {The annals of statistics},
  title     = {{A Bayesian analysis of some nonparametric problems}},
  year      = {1973},
  pages     = {209--230},
  publisher = {JSTOR},
}

@Article{j-Blackwell1973,
  author    = {David Blackwell and James B. MacQueen},
  journal   = {The annals of statistics},
  title     = {{Ferguson distributions via P{\'o}lya urn schemes}},
  year      = {1973},
  number    = {2},
  pages     = {353--355},
  volume    = {1},
  publisher = {Institute of Mathematical Statistics},
}

@Article{j-Sethuraman1994,
  author    = {Jayaram Sethuraman},
  journal   = {Statistica sinica},
  title     = {{A constructive definition of Dirichlet priors}},
  year      = {1994},
  pages     = {639--650},
  publisher = {JSTOR},
}

@Article{j-Aziz2021,
  author  = {Maryam Aziz and Emilie Kaufmann and Marie-Karelle Riviere},
  journal = {Journal of Machine Learning Research},
  title   = {{On Multi-Armed Bandit Designs for Dose-Finding Trials}},
  year    = {2021},
  number  = {14},
  pages   = {1--38},
  volume  = {22},
  url     = {http://jmlr.org/papers/v22/19-228.html},
}

@Misc{deep_bayesian_bandits,
  author = {Carlos Riquelme and George Tucker and Jasper Snoek},
  note   = {Available online at {\url{https://github.com/tensorflow/models/tree/archive/research/deep_contextual_bandits}}},
  title  = {{Deep Bayesian Bandits Library}},
  year   = {2018},
}

@InProceedings{ip-Dong2019,
  author    = {Shi Dong and Tengyu Ma and Benjamin Van Roy},
  booktitle = {Proceedings of the Thirty-Second Conference on Learning Theory},
  title     = {{On the Performance of Thompson Sampling on Logistic Bandits}},
  year      = {2019},
  editor    = {Beygelzimer, Alina and Hsu, Daniel},
  month     = {25--28 Jun},
  pages     = {1158--1160},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {99},
  abstract  = {We study the logistic bandit, in which rewards are binary with success probability $\exp(\beta a^\top \theta) / (1 + \exp(\beta a^\top \theta))$ and actions $a$ and coefficients $\theta$ are within the $d$-dimensional unit ball.  While prior regret bounds for algorithms that address the logistic bandit exhibit exponential dependence on the slope parameter $\beta$, we establish a regret bound for Thompson sampling that is independent of $\beta$.  Specifically, we establish that, when the set of feasible actions is identical to the set of possible coefficient vectors, the Bayesian regret of Thompson sampling is $\tilde{O}(d\sqrt{T})$.  We also establish a $\tilde{O}(\sqrt{d\eta T}/\Delta)$ bound that applies more broadly, where $\Delta$ is the worst-case optimal log-odds and $\eta$ is the “fragility dimension,” a new statistic we define to capture the degree to which an optimal action for one model fails to satisfice for others.  We demonstrate that the fragility dimension plays an essential role by showing that, for any $\epsilon &gt; 0$, no algorithm can achieve $\mathrm{poly}(d, 1/\Delta)\cdot T^{1-\epsilon}$ regret.},
  pdf       = {http://proceedings.mlr.press/v99/dong19a/dong19a.pdf},
  url       = {https://proceedings.mlr.press/v99/dong19a.html},
}

@InProceedings{ip-Faury2020,
  author       = {Louis Faury and Marc Abeille and Cl{\'e}ment Calauz{\`e}nes and Olivier Fercoq},
  booktitle    = {International Conference on Machine Learning},
  title        = {{Improved optimistic algorithms for logistic bandits}},
  year         = {2020},
  organization = {PMLR},
  pages        = {3052--3060},
}

@InProceedings{ip-Yu2009,
  author    = {Jia Yuan Yu and Shie Mannor},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  title     = {{Piecewise-Stationary Bandit Problems with Side Observations}},
  year      = {2009},
  address   = {New York, NY, USA},
  pages     = {1177–1184},
  publisher = {Association for Computing Machinery},
  series    = {ICML '09},
  abstract  = {We consider a sequential decision problem where the rewards are generated by a piecewise-stationary distribution. However, the different reward distributions are unknown and may change at unknown instants. Our approach uses a limited number of side observations on past rewards, but does not require prior knowledge of the frequency of changes. In spite of the adversarial nature of the reward process, we provide an algorithm whose regret, with respect to the baseline with perfect knowledge of the distributions and the changes, is O(k log(T)), where k is the number of changes up to time T. This is in contrast to the case where side observations are not available, and where the regret is at least Ω(√T).},
  doi       = {10.1145/1553374.1553524},
  isbn      = {9781605585161},
  location  = {Montreal, Quebec, Canada},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1553374.1553524},
}

@Article{j-Whittle1988,
  author    = {Peter Whittle},
  journal   = {Journal of Applied Probability},
  title     = {{Restless bandits: activity allocation in a changing world}},
  year      = {1988},
  number    = {A},
  pages     = {287–298},
  volume    = {25},
  doi       = {10.2307/3214163},
  publisher = {Cambridge University Press},
}





@Article{j-Auer2002a,
  author   = {Peter Auer and Nicol\`{o} Cesa-Bianchi and Yoav Freund and Robert E. Schapire},
  journal  = {SIAM Journal on Computing},
  title    = {{The Nonstochastic Multiarmed Bandit Problem}},
  year     = {2002},
  number   = {1},
  pages    = {48-77},
  volume   = {32},
  abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T-1/2 ). We show by a matching lower bound that this is the best possible.We also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((log N1/2T-1/2 ). Finally, we apply our results to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T-1/2 ).},
  doi      = {10.1137/S0097539701398375},
  eprint   = {https://doi.org/10.1137/S0097539701398375},
  url      = {https://doi.org/10.1137/S0097539701398375},
}

@Article{j-Bubeck2012,
  author    = {S{\'e}bastien Bubeck and Nicolo Cesa-Bianchi},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  title     = {{Regret analysis of stochastic and nonstochastic multi-armed bandit problems}},
  year      = {2012},
  number    = {1},
  pages     = {1--122},
  volume    = {5},
  publisher = {Now Publishers, Inc.},
}

@InProceedings{ip-Slivkins2008,
  author    = {Aleksandrs Slivkins and Eli Upfal},
  booktitle = {COLT},
  title     = {{Adapting to a Changing Environment: the Brownian Restless Bandits}},
  year      = {2008},
  pages     = {343--354},
}

@InProceedings{ip-Luo2018,
  author    = {Haipeng Luo and Chen-Yu Wei and Alekh Agarwal and John Langford},
  booktitle = {Proceedings of the 31st Conference On Learning Theory},
  title     = {{Efficient Contextual Bandits in Non-stationary Worlds}},
  year      = {2018},
  editor    = {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  month     = {06--09 Jul},
  pages     = {1739--1776},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {75},
  abstract  = {Most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications.  In this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   We analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. When competing with the best policy at each time, one of our algorithms achieves regret $\mathcal{O}(\sqrt{ST})$ if there are $T$ rounds with $S$ stationary periods, or more generally $\mathcal{O}(\Delta^{1/3}T^{2/3})$ where $\Delta$ is some non-stationarity measure. These results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic Exp4 algorithm. The dynamic regret result is also the first one for efficient and fully adversarial contextual bandit. Furthermore, while the results above require tuning a parameter based on the unknown quantity $S$ or $\Delta$, we also develop a parameter free algorithm achieving regret $\min\{S^{1/4}T^{3/4}, \Delta^{1/5}T^{4/5}\}$. This improves and generalizes the best existing result $\Delta^{0.18}T^{0.82}$ by Karnin and Anava (2016) which only holds for the two-armed bandit problem.},
  pdf       = {http://proceedings.mlr.press/v75/luo18a/luo18a.pdf},
  url       = {https://proceedings.mlr.press/v75/luo18a.html},
}

@InProceedings{ip-Bogunovic2016,
  author    = {Ilija Bogunovic and Jonathan Scarlett and Volkan Cevher},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  title     = {{Time-Varying Gaussian Process Bandit Optimization}},
  year      = {2016},
  address   = {Cadiz, Spain},
  editor    = {Gretton, Arthur and Robert, Christian C.},
  month     = {09--11 May},
  pages     = {314--323},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {51},
  abstract  = {We consider the sequential Bayesian optimization problem with bandit feedback, adopting a formulation that allows for the reward function to vary with time. We model the reward function using a Gaussian process whose evolution obeys a simple Markov model.  We introduce two natural extensions of the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB, instead forgets about old data in a smooth fashion.  Our main contribution comprises of novel regret bounds for these algorithms, providing an explicit characterization of the trade-off between the time horizon and the rate at which the function varies.  We illustrate the performance of the algorithms on both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB to perform favorably compared to the sharp resetting of R-GP-UCB.  Moreover, both algorithms significantly outperform classical GP-UCB, since it treats stale and fresh data equally.},
  pdf       = {http://proceedings.mlr.press/v51/bogunovic16.pdf},
  url       = {https://proceedings.mlr.press/v51/bogunovic16.html},
}

@InProceedings{ip-Jung2019,
  author    = {Young Hun Jung and Ambuj Tewari},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems}},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/2edfeadfe636973b42d7b6ac315b896c-Abstract.html},
}

@Article{j-Jung2019,
  author  = {Young Hun Jung and Marc Abeille and Ambuj Tewari},
  journal = {arXiv preprint arXiv:1910.05654},
  title   = {{Thompson sampling in non-episodic restless bandits}},
  year    = {2019},
  url     = {https://arxiv.org/abs/1910.05654},
}

@Article{j-,
}

@Article{j-Silver2017,
  author    = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  journal   = {nature},
  title     = {{Mastering the game of go without human knowledge}},
  year      = {2017},
  number    = {7676},
  pages     = {354--359},
  volume    = {550},
  publisher = {Nature Publishing Group},
}

@Article{j-a,
}

@Article{j-Mnih2015,
  author    = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal   = {nature},
  title     = {Human-level control through deep reinforcement learning},
  year      = {2015},
  number    = {7540},
  pages     = {529--533},
  volume    = {518},
  publisher = {Nature Publishing Group},
}

@Article{j-Maiz2012,
  author  = {Cristina S. Maiz and Elisa M. Molanes-Lopez and Joaqu\'{i}n Miguez and Petar M. Djuric},
  journal = {IEEE Transactions on Signal Processing},
  title   = {{A Particle Filtering Scheme for Processing Time Series Corrupted by Outliers}},
  year    = {2012},
  number  = {9},
  pages   = {4611-4627},
  volume  = {60},
  doi     = {10.1109/TSP.2012.2200480},
}

@Article{j-Geisser1965,
  author                    = {Seymour Geisser},
  journal                   = {Annals of American Statistics},
  title                     = {{Bayesian-Estimation in multivariate analysis}},
  year                      = {1965},
  issn                      = {{0003-4851}},
  number                    = {1},
  pages                     = {150-159},
  volume                    = {36},
  address                   = {{IMS BUSINESS OFFICE-SUITE 7, 3401 INVESTMENT BLVD, HAYWARD, CA 94545}},
  doi                       = {10.1214/aoms/1177700279},
  language                  = {{English}},
  owner                     = {iurteaga},
  publisher                 = {{INST MATHEMATICAL STATISTICS}},
  research-areas            = {{Mathematics}},
  timestamp                 = {2018.02.01},
  type                      = {{Article}},
  web-of-science-categories = {{Mathematics; Statistics \& Probability}},
}

@Article{j-Geisser1963,
  author              = {Seymour Geisser and Jerome Cornfield},
  journal             = {Journal of the Royal Statistical Society. Series B (Methodological)},
  title               = {{Posterior Distributions for Multivariate Normal Parameters}},
  year                = {1963},
  issn                = {00359246},
  number              = {2},
  pages               = {368-376},
  volume              = {25},
  abstract            = {A class of Bayes posterior distributions is obtained for the parameters of the multivariate normal distribution. These are compared with their fiducial and confidence counterparts.},
  copyright           = {Copyright © 1963 Royal Statistical Society},
  jstor_articletype   = {research-article},
  jstor_formatteddate = {1963},
  keywords            = {Multivariate normal distribution; Bayes posteriors;},
  language            = {English},
  owner               = {iurteaga},
  publisher           = {Wiley for the Royal Statistical Society},
  timestamp           = {2018.02.01},
  url                 = {http://www.jstor.org/stable/2984304},
}

@Article{j-Tiao1964,
  author    = {George C. Tiao and Arnold Zellner},
  journal   = {Journal of the Royal Statistical Society. Series B (Methodological)},
  title     = {{On the Bayesian Estimation of Multivariate Regression}},
  year      = {1964},
  issn      = {00359246},
  number    = {2},
  pages     = {277-285},
  volume    = {26},
  abstract  = {In this paper, we use a Bayesian approach to analyse sets of regression equations with correlated error terms. In the case that the matrix of "independent variables" is the same for all equations, our model reduces to the traditional multivariate regression model. For this case, the marginal posterior distribution of the regression coefficient vector for any equation is shown to be of the multivariate-t form. Further, the variances and covariances of the error terms have an "inverted" Wishart distribution a posteriori. Some properties of this distribution are given. Finally, the joint posterior distribution of the regression coefficients in the general model is derived and discussed.},
  owner     = {iurteaga},
  publisher = {[Royal Statistical Society, Wiley]},
  timestamp = {2018.02.01},
  url       = {http://www.jstor.org/stable/2984424},
}

@InProceedings{Urteaga2018,
  author    = {I{\~n}igo Urteaga and Chris H. Wiggins},
  booktitle = {{1st Symposium on Advances in Approximate Bayesian Inference (AABI)}},
  title     = {{Sequential Monte Carlo for dynamic softmax bandits}},
  year      = {2018},
}

@InProceedings{Urteaga2023,
  author    = {Urteaga, Inigo and Draidia, Moulay Zaidane and Lancewicki, Tomer and Khadivi, Shahram},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  title     = {{Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking}},
  year      = {2023},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  month     = jul,
  pages     = {10609--10627},
  publisher = {Association for Computational Linguistics},
  abstract  = {We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.We propose a multi-armed bandit framework for the sequential selection of pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fewer epochs, across a variety of settings. In addition, GP-TS pre-trained TLMs attain competitive downstream performance, while avoiding expensive hyperparameter grid search. GP-TS provides an interactive framework for efficient and optimized TLM pre-training that, by circumventing costly hyperparameter selection, enables substantial computational savings.},
  doi       = {10.18653/v1/2023.findings-acl.675},
  url       = {https://aclanthology.org/2023.findings-acl.675},
}

@Comment{jabref-meta: databaseType:bibtex;}
